{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fnil\fcharset0 Georgia;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww38200\viewh21120\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs36 \cf0 \ul \ulc0 Problem scenario 1
\b0 \ulnone  : you have been given MySQ1DB with following details\
\
user=retail_dba password=cloudera  database=retail_db table=retail_db.categories jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
\

\b \ul Please accomplish following activities :- 
\b0 \ulnone \
\
1. connect MySQ1DB and check the content of the tables\
2.copy"retail_db.categories"table to hdfs,without specifying directory name\
3.copy "retail_db.categories"table to hdfs, in a directory name "categories_target".\
4.copy "retail_db.categories"table to hdfs, in a warehouse directory name"categories_warehouse".\
\

\b \ul Solution:-
\b0 \ulnone \
\
Step 1 : Connecting to existing MySQ1Database\
mysq1--user=retail_dba --password=cloudera retail_db\
\
Step 2 :Show al1the available tables\
Show tables;\
\
Step 3 : View/Count data from a table in MySQL\
select count(1)from categories;\
\
Step 4 :Check the currently available data in HDFS directory\
hdfs dfs -ls\
\
Step 5 :Import Single table(Without specifying directory).\
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudrea -table=categories\
\
Note:Please check you don\'92t have space between before or after\'92=\'92sign.\
swoop uses the MapReduce framework to copy data from RDMS to hdfs\
\
Step 6:Read the data from one of the partition created using above command.\
hdfs dfs -cat categories/part-m-00000\
\
Step 7:Specifying target directory in import command(We are using number of mappers=1, you can change accordingly)\
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table=categories --target-dir=categories_target --m 1\
\
Step 8:Check the connect in one of the partition file.\
hdfs dfs -cat categories_target/part-m-00000\
\
Step 9:Specifying parent directory so that you can copy more than one table in a specified target directory.Command to specify warehouse directory\
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table=categories --warehouse-dir=categories_warehouse --m 1\
\
Step 10:See the content in one of the file(partition)\
hdfs dos -cat categories_warehouse/categories/part-m-00000\
**************************************************************************************************************************************------------------------------------------\

\b \ul problem scenario 2: 
\b0 \ulnone \
\
There is a parent organisation called "Acmeshel1Group Inc", which has two child companies named HadoopThoughtworks Inc and Hadoopthoughtworks Inc.\
Both companies employee information is given in two separate text file as below,Please do the following activity for the employee details.\
\
HadoopThoughtworks.txt\
1,Alok,Hyderabad\
2,Krish,Hongkong\
3,Jyoti,Mumbai\
4,Atul,Banglore\
5,Ishan,Gurgaon\
\
hadoopthoughtworks.txt\
6,John,Newyork\
7,alp2004,California\
8,tellme,Mumbai\
9,Gagan21,Pune\
10,Mukesh,Chennai\
\
1.Which command wil1you use to check al1the available command line options on HDFS and how wil1you get the help of individua1command.\
2.Create new empty Directory named Employee using command line. And also create an empty file named in it HadoopThoughtworks.txt\
3.load both companies Employee data in Employee directory (How to override existing file in HDFS).\
4,Merge both the employees data in a single file called MergeEmployee.txt, merged files should have new line character at the end of each file content.\
5.Upload merged file on HDFS and change the file permission on HDFS merged file, so that owner and group number can read and write, other user can read the file.\
6.write a command to export the individua1file as wel1as entire directory from HDFS to loca1file sytem.\
----------------------------------------------------------------------------------------------------------------------\
\

\b \ul Solution:-
\b0 \ulnone \
\

\b \ul Step 1:Check Al1Available command
\b0 \ulnone \
hdfs dfs\

\b \ul Step 2:Get help on Individua1command
\b0 \ulnone \
hdfs dfs -help get\

\b \ul Step 3:Create a directory in HDFS using named Employee and create a Dummy file in it called e.g.HadoopThoughtworks.txt
\b0 \ulnone \
hdfs dfs -mkdir Employee\
Now create an empty file in Employee directory using Hue(As shown in video)\
Step 4:Create a directory on Loca1file System and then Create two files,with the given data in problems.\
Step 5:Now we have an existing directory with content in it,now using Hdfs command line,override this existing Employee directory.\
While copying these files loca1file System to HDFS.\
cd/home/cloudera/Desktop/\
hdfs dfs -put -f Employee\

\b \ul Step 6:Check Al1files in directory copied successfully
\b0 \ulnone \
hdfs dfs -ls Employee\

\b \ul Step 7:Now merge al1the files in Employee directory.
\b0 \ulnone \
hdfs dfs -getmerge -n1Employee MergedEmployee.txt\

\b \ul Step 8:check the content of the file
\b0 \ulnone .\
cat MergedEmployee.txt\

\b \ul Step 9:Copy merged file in Employee directory from loca1file system to HDFS
\b0 \ulnone .\
hdfs dfs -put MergedEmployee.txt Employee/\

\b \ul Step 10:Check file copied or not
\b0 \ulnone \
hdfs dfs -ls Employee\

\b \ul Step 11:Change the permission of the merged file on HDFS
\b0 \ulnone \
hdfs dfs -chmod 664 Employee/MergedEmployee.txt\

\b \ul Step 12:Get the file from Hdfs to loca1file system
\b0 \ulnone \
hdfs dfs -get Employee Employee_hdfs\
\
**************************************************************************************************************************************------------------------\

\b \ul \
Problem Scenario 3
\b0 \ulnone : You Have been given MySQ1DB with following details.\
\
user=retail_dba  password=cloudera  database=retail_db table=retail_db.categories jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
\
Please accomplish following activities.\
\
1.Import data from categories table, where category=22 (Data shout be stored in categories_subset)\
2.Import data from categories table, where category>22 (Data should be stored in categories_subset_2)\
3.Import data from categories table, between 1 and 22 (Data should be stored in categories_subset_3)\
4.While importing categories data change the delimiter to '|' (Data should be stored in categories_subset_6)\
5.Importing data from categories table and restrict the import to category_name,category_id columns only with delimiter as '|'\
6.Add nul1values in the table using below SQ1statement\
AL1TER TABLE categories modify category department_id int(11);\
7.Importing data from categories table (In categories_subset_17 directory) using '|' delimiter and category_id between 1 and 61 and encode nul1values for both string and non string columns.\
8.Import entire schema retail_db in a directory categories_subset_all_tables\
\

\b \ul Solution:- \

\b0 \ulnone \
Step 1: Import Single table (Subset data)\
\
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table=categories --warehouse-dir=categories_subset --where \'92category_id\'92=22 --m 1\

\b \ul Step 2 : Check the output partition 
\b0 \ulnone \
hdfs dfs -cat categories_subset/categories/part-m-OOOOO \

\b \ul Step 3 : Change the selection criteria (Subset data) 
\b0 \ulnone \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table=categories --warehouse-dir=categories_subset_2 --where \'91category_id\'92>22 --m 1\

\b \ul Step 4 : Check the output partition 
\b0 \ulnone \
hdfs dfs -cat categories_subset_2/categories/part-m-OOOOO \

\b \ul Step 5 : use between clause (Subset data)
\b0 \ulnone  \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table=categories --warehouse-dir=categories_subset_3 --where \'93category_id between 1 and 22\'94 --m 1 \

\b \ul Step 6 : Check the output partition
\b0 \ulnone  \
hdfs dfs -cat categories_subset_3/categories/part-m-OOOOO \

\b \ul Step 7 : Changing the delimiter during import. 
\b0 \ulnone \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table=categories --warehouse-dir=categories_subset_6 --where \'93category_id between 1 and 22\'94 --fields-terminated-by=\'91|\'92 --m1 \

\b \ul Step 8 : Check the output partition 
\b0 \ulnone \
hdfs dfs -cat categories_subset_6/categories/part-m-OOOOO \

\b \ul Step 9 : Selecting subset columns 
\b0 \ulnone \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera--table=categories --warehouse-dir=categories_subset_co1--where \'93category_id between 1 and 22\'94--fields-terminated-by=\'91|\'92 --columns=category_name,category_id --m 1 \

\b \ul Step 10 : Check the output partition 
\b0 \ulnone \
hdfs dfs -cat categories_subset_7/categories/part-m-OOOOO \

\b \ul Step 11 : Inserting record with nul1values (Using mysql) 
\b0 \ulnone \
ALTER TABLE categories modify category _ department_id int(11); \
INSERT INTO categories values select * from categories; \

\b \ul Step 12 : Encode non string nul1column 
\b0 \ulnone \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table=categories --warehouse-dir=categories_subset_17 --where \'93category_id between 1 and 61\'94 --fields-terminated-by=\'91|\'92 --null-string=\'92N\'92 --null-non-string=\'92N\'92 --m1 \

\b \ul Step 13 : View the content 
\b0 \ulnone \
hdfs dfs -cat categories_subset_17/categories/part-m-OOOOO \
Step 14 : Import al1the tables from a schema (This step wil1take little time) \
ALTER TABLE categories modify category_department_id int(11) NOT NULL; \
ALTER TABLE categories modify category_name varchar(45) NOT NULL; \
desc categories; \
**************************************************************************************************************************************----\

\b \ul Problem Scenario 4
\b0 \ulnone : You have been given MySQ1DB with following details\
\
user=retail_dba password=cloudera  database=retail_db table=retail_db.categories jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Please accomplish following activities.\
Import single table categories(subset data) to hive managed table, where category_id between 1 and 22\
\

\b \ul Solution:
\b0 \ulnone \
Step 1 : Import Single table (Subset data) \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table=categories --where\'94 \'92category_id \'92between 1 and 22\'94 --hive-import --m1\
Note: Here the \'95 is the same you find on -- key \
This command wil1create a managed table and content wil1be created in the following directory. \
/user/hive/warehouse/categories \
Step 2 : Check whether table is created or not (In Hive) \
show tables; \
select * from categories;\
----------------------------------------------------------\
Problem Scenario 5: You have been given following data format file. Each datapoint is separated by '|'\
Name|Location1,Location2..Location5|Sex,Age|Father_Name:Number_of_child\
thoughtworksple Record\
Anupam|Delhi,Mumbai,Chennai|Male,45|Daulat:4\
write a Hive DD1script to create a table named "FamilyHead" which should be capable of holding these data. Also note that it should use complex data type e.g. Map,Array,Sttuct\
----------------------------------------------------------------------------------------------------------------------------\

\b \ul Solution:
\b0 \ulnone \
CREATE TABLE FamilyHead \
(\
name string, \
business _ places ARRAY<string>, \
sex _ age STRUCT<sex:string,age:int>, \
fatherName_NuOtChild MAP<string,int> \
)\
ROW FORMAT DELIMITED \
FIELDS TERMINATED BY \'91|\'92 \
COLLECTION ITEMS TERMINATED BY \'91,\'92\
MAP KEYS TERMINATED BY \'91.\'92;\
----------------------------------------------------------------\
Problem Scenario 6: You have been given following data format file. Each datapoint is separated by '|\
Name|Sex|Age|Father_Name\
thoughtworksple Record\
Anupam|Male|45|Daulat\
Create an Hive database named "Family" with following details. you must take care that if database is already exist it should not be created again.\
Comment: "This database wil1be used for collecting various family data and their daily habits"\
Data File Location: '/hdfs/family'\
Stored other properties: "'Database Creator'=Vedic"', "'Database_Created_On'='2016-01-01"'\
Also write a command to check, whether database has been created or not, with new properties.\
\

\b \ul Step 1 : Create database
\b0 \ulnone  \
CREATE DATABASE IF NOT EXISTS Family \
COMMENT 'This database wil1be used tor collecting various family data and their daily habits' \
LOCATION \'91/hdfs/family\'92 \
WITH DBPROPERTIES (\'91Database creator\'92=\'91Database_Created_On\'92=\'922016-01-01\'92);\

\b \ul Step 2 : Check the database 
\b0 \ulnone \
SHOW DATABASES; \
DESCRIBE DATABASE extended Family;\
--------------------------------------------------------------\

\b \ul Problem Scenario 7
\b0 \ulnone : You have been given following data format file. Each datapoint is separated by '|'.\
\
Name|Sex|Age|Father_Name\
Anupam|Male|45|Daulat\
Create an Hive database named "Family" with following details. \
-Table must be created in existing database named "Family"\
-you must take care that if table is already exist it should not be created again.\
-Table must be created inside hive warehouse directory and should not be an externa1table.\
\

\b \ul Solution:
\b0 \ulnone \
\

\b \ul Step 1. use the existing database \

\b0 \ulnone USE Family \

\b \ul Step 2. Create table 
\b0 \ulnone \
CREATE TABLE IF NOT EXISTS Family_Head \
(\
name string, \
business _ places ARRAY<string>, \
sex _ age STRUCT<sex:string,age:int>, \
fatherName_NuofChild MAP<string,int> \
)\
ROW FORMAT DELIMITED \
FIELDS TERMINATED BY \'91|\'92 \
COLLECTION ITEMS TERMINATED BY \
MAP KEYS TERMINATED BY \'91:\'92;\

\b \ul Step 3: Describe the created table 
\b0 \ulnone \
DESC Family_Head;\
------------------------------------------------------------------------\

\b \ul Problem Scenario 8
\b0 \ulnone  : You already have a table name " SAMPLE_07" in a default schema. with following structure.\
\
Code String\
Description String\
total_emp int\
Salary Int\
\
Sample data as below\
Sample_07.code sample_07.description sample_07.total_emp sample_07.salary\
00-0000 Al1Occupations 134354250 40690\
11-0000 Management occupations 6003930 96150\
\
Create another table named Employee 100K in a Family Schema, Which has al1the employees whose salary is >=100000\
Also write a simple query to verity Salary> 100k does not exist in new table.\
\

\b \ul Solution:
\b0 \ulnone \
CREATE TABLE Family.Employee100K AS SELECT* FROM sample_07 where salary>=100000; \
SELECT * FROM Family.Employee100K Where salary < 100000\
----------------------------------------------------------\

\b \ul Problem Scenario 9
\b0 \ulnone  : You already have two table (HIVE)name " SAMPLE_07" and "SAMPLE_08" in a default schema. with following structure (bothe the tables have s ame structure).\
Code String\
Description String\
total_emp int\
Salary Int\
Create another table named Employee 100K2 in a Family Schema, Which has al1the employees whose salary is >=100000 from the both tables\

\b \ul Solution:
\b0 \ulnone \
create database family; \
use family; \
CREATE TABLE Family.Employee100K2 AS WITH dataset1 AS (SELECT* FROM sample_07 where salary >=1OOOOO),  dataset2 AS (SELECT* FROM sample_08 where salary >=IOOOOO) \
SELECT * FROM dataset1 UNION AL1select * FROM dataset2; \
SELECT * FROM Family.Employee100K2;\
----------------------------------------------------------------------------\

\b \ul Problem Scenario 10 : you have been given a database named retail_db with following detail. which consists 6 tables and datamode1you can see in image. ( Had Image)
\b0 \ulnone \
User=retail_data password=cloudera database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
1.Import the entire database in a file format this good for analytica1applications on hadoop e.g. group your data in columns and should be able to query this data using Impala.\
Also, while importing to save space you do compression using snappy codec.\
2.In impala write the query, which can produce 5 most popular product categories and save the results in hadoopthoughtworks/best_categories.csv in hdfs.\
3.In impala write the query, which can produce top 10 revenue generating products and save the results in hadoopthoughtworks/best_products.csv in hdfs.\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : First make sure you don\'92t have already imported tables in hive. It you have done than please drop the same. 
\b0 \ulnone \
drop table employee; \

\b \ul Step 2 : using Sqoop Import entire schema, the file format wil1be parquet, because that is the one which group your data in columns, while storing
\b0 \ulnone \
sqoop import-all-tables --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba ---password=cloudera --compression-codec=snappy --as-parquetfile --warehouse-dir=/user/hive/warehouse --hive-import -m 1 \

\b \ul Step 3 : Verity the import.
\b0 \ulnone  \
hadoop dfs -ls /user/hive/warehouse/ \
hadoop dfs -ls /user/hive/warehouse/categories/ \

\b \ul Step 4 : By default Impala does not know, its metadata has been updated. Hence first make Impala aware about metadata change and update the same.
\b0 \ulnone \
show tables; \
invalidate metadata; \

\b \ul Step 5 : Now check al1the tables are available or not in Impala. 
\b0 \ulnone \
show tables; \

\b \ul Step 6 : Write a query and execute it to find most 5 popular product categories.
\b0 \ulnone  \
select c.category_name, count(order_item_quantity) as count \
from order_items oi \
inner join products p on oi.order_item_product_id = p.product_id \
inner join categories c on c.category_id = p.product_category_id \
group by c.category_name \
order by count desc \
limit 5; \

\b \ul Step 7 : Write a query to find, top 10 revenue generating products
\b0 \ulnone  \
select p.product_id, p.product_name, r.revenue \
from products p inner join \
(select oi.order_item_product_id, sum(cast(oi.order_item_subtota1as float)) as revenue \
from order _ items oi inner join orders o \
on oi.order_item_order_id = o.order_id \
where o.order status <> 'CANCELED' and o.order status <>\'92SUSPECTED_FRAUD' group by order_item_product_id) r on \
p.product_id = r.order_item_product_id order by r.revenue desc limit 10;\
------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 11:
\b0 \ulnone  you have been given following mysq1database details.\
user=retail_dba password=cloudera database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
\
Please accomplish following activities.\
\
1.List al1the tables using sqoop command from retail_db\
2.Write simple sqoop eva1command to check whether you have permission to read database tables or not.\
3.Import al1the tables as avro files in /user/hive/warehouse/retail_cca174.db\
4.Import departments table as a text file in /user/cloudera/departments.\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : List tables using sqoop 
\b0 \ulnone \
sqoop list-tables --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera \

\b \ul Step 2 : Eva1command, just run a count query on one of the table
\b0 \ulnone . \
sqoop eva1 --connect jdbc:mysql://quickstart:3306/retail_db  --username retail_dba  --password=cloudera  --query "select count(l) from order_items" \

\b \ul Step 3 : Import al1the tables as avro file.
\b0 \ulnone  \
sqoop import-all-tables  --connect jdbc:mysql://quickstart:3306/retail_db  --username--retail_dba  --password=cloudera  --as-avrodatatfle  --warehouse-dir=/user/hive/warehouse/retai1stage.db  \'97m 1\

\b \ul Step 4 : Import departments table as a text tile in /user/cloudera/departments
\b0 \ulnone  \
sqoop import  --connect jdbc:mysql://quickstart:3306/retail_db  --username--retail_dba  --password=cloudera  --table departments  --as-textile  --target-dir=/user/cloudera/departments \

\b \ul Step 5 : Verity the imported data.
\b0 \ulnone  \
hdfs dfs -ls /user/cloudera/departments \
hdfs dfs -ls a/user/hive/warehouse/retail_stage.db \
hdfs dfs -ls /user/hive/warehouse/retail_stage.db/products\
--------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 12
\b0 \ulnone : you have been given following mysq1database details as wel1as other info\
user=retail_dba  password=cloudera database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db Compression Codec : org.apache.hadoop.io.compress.SnappyCodec\
Please accomplish following.\
1.Import entire database such that it can be used as a hive tables, it must be created in default schema.\
2.Also make sure each tables file is partitioned in 3 files e.g. part-00000, part-00002, part-00003\
3.Store al1the Java file in a directory called java_output to evaluate the further\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Drop al1the tables, which we have created in previous problems. Before implementing the solution. 
\b0 \ulnone \
Login to hive and execute following command. \
show tables; \
drop table categories; \
drop table customers; \
drop table departments; \
drop table employee; \
drop table order _ items; \
drop table orders; \
drop table products; \
show tables; \
Check warehouse directory. \
hdfs dfs -ls /user/hive/warehouse \

\b \ul Step 2 : Now we have cleaned database. Import entire retai1db with al1the required parameters as problem statement is asking. 
\b0 \ulnone \
sqoop import-all-tables --m 3 --connect  jdbc:mysql://quickstart:3306/retail_db --username=retail_dba  --password=cloudera --hive-import --hive-overwrite --create-hive-table --compress --compression-codec org.apache.hadoop.io.compress.snappycodec --outdir java_output \

\b \ul Step 3 : Verify the work is accomplished or not
\b0 \ulnone . \

\b \ul a. Go to hive and check al1the tables 
\b0 \ulnone \
hive \
show tables;\
select count(1) from customers; \

\b \ul b. Check the warehouse directory and number of partitions. 
\b0 \ulnone \
hdfs dfs -ls luser/hive/warehouse\
hdfs dfs -ls /user/hive/warehouse/categories \

\b \ul c. Check the output java directory. 
\b0 \ulnone \
ls -ltr java _ output/\
----------------------------------------------------------------------------------------\

\b \ul Problem Scenario 13:
\b0 \ulnone  you have been given following mysq1database details as wel1as other info\
user=retail_dba  password=cloudera  database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Please accomplish following.\
1. Import department tables using your customer boundary query, which import departments between 1 to 25.\
2. Also make sure each tables file is partitioned in 2 files e.g. part-00000, part-00002\
3. Also make sure \ul you have imported only two columns from table, which are department_id,department_name\
\ulnone \

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Clean the hdfs file system, it they exists clean out.
\b0 \ulnone  \
hadoop fs -rm -R departments \
hadoop fs -rm -R categories \
hadoop fs -rm -R products \
hadoop fs -rm -R orders \
hadoop fs -rm -R order_items \
hadoop fs -rm -R customers \

\b \ul Step 2 : Now import the department table as per requirement.
\b0 \ulnone  \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba  --password=cloudera  --table departments  --target-dir /user/cloudera/departments  --m 2 --boundary-query "select 1, 25 from departments"  --columns department_id,department_name 
\b \ul \
Step 3 : Check imported data
\b0 \ulnone . \
hdfs dfs -Is departments \
hdfs dfs -cat departments/part-m-OOOOO \
hdfs dfs -cat departments/part-m-00001\
--------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 14:
\b0 \ulnone  you have been given following mysq1database details as wel1as other info\
\
user=retail_dba password=cloudera database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
\
Please accomplish following.\
1. Import Joined result of orders and order_items table join on orders.order_id = order_items.order_item_order_id.\
2.Also make sure each tables file is partitioned in 2 files e.g. part-00000, part-00002\
3.Also make sure you use order_id columns for sqoop to use for boundary conditions.\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Clean the hdfs tile system, it they exists clean out.
\b0 \ulnone  \
hadoop fs -rm -R departments \
hadoop fs -rm -R categories \
hadoop fs -rm -R products \
hadoop fs -rm -R orders \
hadoop fs -rm -R order_items \
hadoop fs -rm -R customers \

\b \ul Step 2 : Now import the department table as per requirement.
\b0 \ulnone  \
sqoop import --connect \'93jdbc:mysql://quickstart:3306/retail_db\'94 --username=retail_dba --password=cloudera --query=\'93select * from orders join order_items on orders.order_id = order items.order item order id where  $CONDITIONS\'94 --target-dir /user/cloudera/order_join --split-by order_id --num-mappers 2 \

\b \ul Step 3 : Check imported data.
\b0 \ulnone  \
hdfs dfs -Is order_join \
hdfs dfs -cat order Join/part-m-OOOOO \
hdfs dfs -cat order Join/part-m-00001\

\b ----------------------------------------------------------------------------------------------------------------------------\
\ul Problem Scenario 15:
\b0 \ulnone  you have been given following mysq1database details as wel1as other info\
\
user=retail_dba password=cloudera  database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Please accomplish following.\
1.Import departments table in a directory.\
2.Again import departments table same directory (However, directory already exist hence it should not override and append the results)\
3. Also make sure your results fields are terminated by '|' and lines terminated by ' n'.\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Clean the hdfs tile system, it they exists clean out. 
\b0 \ulnone \
hadoop fs -rm -R departments \
hadoop fs -rm -R categories \
hadoop fs -rm -R products \
hadoop fs -rm -R orders \
hadoop fs -rm -R order_items \
hadoop fs -rm -R customers \

\b \ul Step 2 : Now import the department table as per requirement.
\b0 \ulnone  \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db  --username--retail_dba --password=cloudera --table departments --target-dir=departments --fields-terminated-by \'91|\'92  --lines-terminated-by \'91 n\'92   --m 1\

\b \ul Step 3 : Check imported data. 
\b0 \ulnone \
hdfs dfs -ls departments \
hdfs dfs -cat departments/part-m-OOOOO \

\b \ul Step 4 : Now again import data and needs to appended.
\b0 \ulnone  \
sqoop import  --connect jdbc:mysql://quickstart:3306/retail_db  --username--retail_dba  --password=cloudera  --table departments  --target-dir departments  --append  --fields-terminated-by \'91|\'92 --lines-terminated-by ' n'  \

\b \ul Step 5 : Again Check the results
\b0 \ulnone  \
hdfs dfs -Is departments \
hdfs dfs -cat departments/part-m-00001\
----------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 16
\b0 \ulnone : you have been given following mysq1database details as wel1as other info\
user=retail_dba \
password=cloudera \
database=retail_db\
jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Please accomplish following.\
1.Create a database named hadoopthoughtworks and then create a table named departments in it, with following field,\
department_id int\
department_name string\
e.g location should be hdfs://quickstart.cloudera:8020/user/hive/warhouse/hadoopthoughtworks.db/departments\
2.Please import data in existing table created above from retai_db.departments into hive table hadoopthoughtworks.departments.\
3.Please import data in a non-existing table, means while Importing create hive table named hadoopthoughtworks.departments_new.\

\b \ul Solution : 
\b0 \ulnone \
\

\b \ul Step 1 : Go to hive interface and create database. 
\b0 \ulnone \
hive \
create database hadoopthoughtworks; \

\b \ul Step 2. use the database created in above step and then create table in it. 
\b0 \ulnone \
use hadoopthoughtworks; \
show tables; \

\b \ul Step 3 : Create table in it. 
\b0 \ulnone \
create table departments (department_id int,department_name string);\
show tables; \
desc departments; \
desc formatted departments; \

\b \ul Step 4 : Please check following directory must not exist else it wil1give error.
\b0 \ulnone  \
hdfs dfs -ls /user/cloudera/departments \
It directory already exists, make sure it is not usefu1and than delete the same. \
This is the staging directory where Sqoop store the intermediate data before pushing in hive table. \
hadoop fs -rm -R departments \

\b \ul Step 5 : Now import data in existing table 
\b0 \ulnone \
sqoop import  --connect jdbc:mysql://quickstart:3306/retail_db  --username=retail_dba  --password=cloudera --table departments --hive -home/user/hive/warehouse --hive-import --hive-overwrite --hive-table hadoopthoughtworks.departments\

\b \ul step 6: Check whether data has been loaded or not.
\b0 \ulnone \
hive;\
use hadoopthoughtworks;\
show tables;\
select * from departments;\
desc formatted departments;\

\b \ul Step 7:Import data in non-existing tables in hive and create table while importing
\b0 \ulnone \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table departments --hive-home/user/hive/warehouse --hive-import --hive-overwrite --hive-table hadoopthoughtworks.departments_new --create-hive-table\

\b \ul Step8:Check whether data has been loaded or not.
\b0 \ulnone \
hive;\
use hadoopthoughtworks;\
show tables;\
select * from departments_new;\
desc formatted departments_new;\
--------------------------------------------------------------------------------\

\b \ul Problem Scenario 17:
\b0 \ulnone  you have been given following mysq1database details as wel1as other info\
user=retail_dba password=cloudera database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Please accomplish following\
1.Import departments table in a directory called departments.\
2.Once import is done, please insert following 5 records in departments mysq1table.\
Insert into departments(10, Physics);\
Insert into departments(11, Chemistry);\
Insert into departments(12, Maths);\
Insert into departments(13, Science);\
Insert into departments(14, Engineering);\
3.Now Import only new inserted records and append to existing directory, which has been created in first step.\
\

\b \ul Solution : 
\b0 \ulnone \
\

\b \ul Step 1 :
\b0 \ulnone  Clean already imported data. (In rea1thought works, please make sure you don\'92t delete data generated from previous exercise). \
hadoop fs -rm -R departments \

\b \ul Step 2 : Import data in departments directory
\b0 \ulnone . \
sqoop import  --connect jdbc:mysql://quickstart:3306/retail_db  \'97username=retail_dba  --password=cloudera  --table departments  --target-dir /user/cloudera/departments \

\b \ul Step 3 : Insert the five records in departments table. 
\b0 \ulnone \
mysq1--user=retail_dba---password=cloudera retail_db \
Insert into departments values(10, "physics"); \
Insert into departments values(11, "Chemistry"); \
Insert into departments values(12, "Maths"); \
Insert into departments values(13, "Science"); \
Insert into departments values(14, "Engineering"); \
commit; \
select * from departments; \

\b \ul Step 4 : Get the maximum value of departments from last import.
\b0 \ulnone  \
hdfs dfs -cat /user/cloudera/departments/part*\
that should be 7\

\b \ul Step5 :Do the incrementa1import based on last import and append the results.
\b0 \ulnone \
sqoop import  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db"  --username=retail_dba  --password=cloudera  --table departments  --target-dir /user/cloudera/departments  --append  --check-column "department_id"  --incrementa1append  --last-value 7 \

\b \ul Step 6 : Now check the result. 
\b0 \ulnone \
hdfs dfs -cat /user/cloudera/departments/part*\
----------------------------------------------------------------------------------------------\
Problem Scenario 18: you have been given following mysq1database details as wel1as other info\
user=retail_dba password=cloudera database=retail_dba jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
\
Please accomplish following.\
1. Create a table in  retail_db with following definition\
CREATE table departments_new (department_id int(11), department_name varchar(45), created_date TIMESTAMP DEFAULT NOW());\
2. Now insert records from departments table  to departments_new\
3. Now import data from departments_new table to hdfs.\
4. Insert following 5 records in departments_new table.\
Insert into departments_new values(110, "Civil" , null);\
Insert into departments_new values(111, "Mechanical" , null);\
Insert into departments_new values(112, "Automobile" , null);\
Insert into departments_new values(113, "Pharma" , null);\
Insert into departments_new values(114, "Socia1Engineering" , null);\
5. Now do the incrementa1import based on created_date column.\
------------------------------------------------------------------------------\

\b \ul Solution : \

\b0 \ulnone \

\b \ul Step 1 : Login to mysq1db 
\b0 \ulnone \
mysq1--user=retail_dba --password=cloudera \
show databases; \
use retail_db; \
show tables; \

\b \ul Step 2 : Create a table as given in problem statement.
\b0 \ulnone  \
CREATE table departments_new (department_id int(11), department_name varchar(45), created_date TIMESTAMP DEFAULT NOW()); \
show tables; \

\b \ul Step 3 : insert records from departments table to departments _ new 
\b0 \ulnone \
insert into departments_new select a.*, nul1from departments a; \

\b \ul Step 4 : Import data from departments_new table to hdfs
\b0 \ulnone . \
sqoop import  --connect jdbc:mysql://quickstart:3306/retail_db  --username=retail_dba  --password=cloudera  --table departments_new  --target-dir /user/cloudera/departments_new  --split-by department_id \

\b \ul Step 5 : Check the imported data
\b0 \ulnone . \
dfs dfs -cat /user/cloudera/departments_new/part*\

\b \ul Step 6: Insert following 5 records in departments _ new table. 
\b0 \ulnone \
Insert into departments_new values(110, "Civil" , null); \
Insert into departments_new values(lll, "Mechanical" , null);\
Insert into departments_new values(112, "Automobile" , null); \
Insert into departments_new values(113, "Pharma" , null); \
Insert into departments_new values(114, "Socia1Engineering" , null); \
commit; \

\b \ul Step 7 : Import incrementa1data based on created_date column.
\b0 \ulnone  \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table departments _ new --target-dir /user/cloudera/departments_new --append --check-column created_date --incrementa1last-modified --split-by department_id --last-value "2016-01-30  12:07:37.0\'94\

\b \ul Step 8 : Check the imported value.
\b0 \ulnone  \
hdfs dfs -cat /user/cloudera/departments_new/part*\
----------------------------------------------------------------\

\b \ul Problem Scenario 19
\b0 \ulnone : you have been given following mysq1database details as wel1as other info\
\
user=retail_dba password=cloudera database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Please accomplish following.\
\
1. Create a table in retail_db with following definition.\
CREATE table departments_export (department_id int(11), department_name varchar(45), created_date TIMESTAMP DEFAULT NOW());\
2. Now import the data from following directory into departments_export table.\
/user/cloudera/departments_new\
\

\b \ul Solution : 
\b0 \ulnone \

\b \ul Step 1 : Login to mysq1db
\b0 \ulnone  \
mysq1--user=retail_dba --password=cloudera \
show databases; \
use retai1db; \
show tables; \

\b \ul Step 2 : Create a table as given in problem statement.
\b0 \ulnone  \
CREATE table departments_export (department_id int(11), department_name varchar(45), created_date TIMESTAMP DEFAULT NOW()); \
show tables; \

\b \ul Step 3 : Export data from /user/cloudera/departments_new to new table departments_export 
\b0 \ulnone \
sqoop export --connect jdbc:mysql://quickstart:3306/retail_db  --username retail_dba --password cloudera --table departments_export  --export-dir /user/cloudera/departments_new --batch \

\b \ul Step 4 : Now check the export is correctly done or not.
\b0 \ulnone  \
mysq1--user=retail_dba --password=cloudera \
show databases; \
use retail_db; \
show tables; \
select * from departments_export;\
------------------------------------------------------------------------------------\

\b \ul Problem Scenario 20
\b0 \ulnone : you have been given following mysq1database details as wel1as other info\
user=retail_dba password=cloudera database=retail_dba jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Please accomplish following.\
\
1.Create a csv file named updated_departments.csv with the following contents in loca1file system\
2,fitness\
3,footwear\
12,fathematics\
13,fscience\
14,engineering  \
1000,management\
\
2.Upload this csv file to hdfs filesystem,\
3. Now export this data from hdfs to mysq1retail_db.departments table. During upload make sure existing department wil1just updated and new departments needs to be inserted.\
4.Now update updated_departments.csv file with below content.\
\
2,Fitness\
3,Footwear\
12,Fathematics\
13,Science\
14,Engineering  \
1000,Management\
2000,Quality Check\
\
5.Now upload this file to hdfs.\
6.Now export this data from hdfs to mysq1retail_db.departments table. During upload make sure existing department wil1just updated and no new departments needs to be inserted.\
\

\b \ul Solution : 
\b0 \ulnone \
\
Step 1 : Create a csv file named updated_departments.csv with given content. \
\

\b \ul Step 2 : Now upload this tile to HDFS. 
\b0 \ulnone \
Create a directory called new_data. \
hdfs dfs -mkdir new data \
hdfs dfs -put updated_departments.csv new_data/\

\b \ul Step 3 : Check whether file is uploaded or not. 
\b0 \ulnone \
hdfs dfs -ls new_data \

\b \ul Step 4 : Export this file to departments table using sqoop. 
\b0 \ulnone \
sqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba--password cloudera --table departments --export-dir new_data --batch  -m 1 --update-key department_id --update-mode allowinsert \

\b \ul Step 5 : Check whether required data insert is done or not.
\b0 \ulnone  
\b \ul \

\b0 \ulnone mysq1 --user=retail_dba --password=cloudera \
show databases; \
use retail_db; \
show tables; \
select * from departments;\
Step 6 : update updated_departments.csv file.\

\b \ul Step 7 : Override the existing tile in hdfs.
\b0 \ulnone  \
hdfs dfs -put updated_departments.csv new_data\

\b \ul Step 8 : Now do the Sqoop export as per the requirement. 
\b0 \ulnone \
sqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera--table departments  --export-dir new_data --batch --m 1 --update-key department_id --update-mode updateonly \

\b \ul Step 9 : Check whether required data update is done or not. 
\b0 \ulnone \
mysq1--user=retail_dba --password=cloudera \
show databases; \
use retail_db; \
show tables; \
select * from departments;\
----------------------------------------------------------------------------\

\b \ul Problem Scenario 21:
\b0 \ulnone  you have been given following mysq1database details as wel1as other info\
\
user=retail_dba password=cloudera database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
\
Please accomplish following activities.\
\
1. In mysq1departments table please insert following record.\
Insert into departments values(9999, "'Data Science"");\
2.Now there is a downstream system which wil1process dumps of this file. However, system is designed  the way that it can process only files if fields are enclosed in(')single quote and separater of the field should be (~) and line needs to be terminated by : (colon).\
3.If data itself contains the " (double quote ) than it should be escaped by  .\
4.Please import the departments table in a directory called departments_enclosedby and file should be able to process by downstream system\
\

\b \ul Solution : 
\b0 \ulnone \
\

\b \ul Step 1 : Connect to mysq1database.
\b0 \ulnone  \
mysq1--user=retail_dba ---password=cloudera \
show databases; \
use retail_db; \
show tables; \
Insert record \
Insert into departments values(9999, '"Data Science"' ); \
select * from departments; \

\b \ul Step 2 : Import data as per requirement.\ulnone  
\b0 \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --table departments --target-dir /user/cloudera/departments_enclosedby --enclosed-by  \'92--escaped-by   --fields-terminated-by=\'91~\'92-lines-terminated-by:\

\b \ul Step 3 : Check the result.
\b0 \ulnone  \
hdfs dfs -cat /user/cloudera/departments_enclosedby/part*\
------------------------------------------------------------------\

\b \ul Problem Scenario 22
\b0 \ulnone : you have been given following mysq1database details as wel1as other info\
user=retail_dba  password=cloudera database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Please accomplish below assignment.\

\b \ul 1.Create a table in hive as below.
\b0 \ulnone \
Create table departments_hive(department_id int, department_name string);\

\b \ul 2. Now import data from mysq1table departments to  this hive table. please make sure that data should be visible using below hive command.
\b0 \ulnone \
Select * from departments_hive\
\

\b \ul Solution : 
\b0 \ulnone \

\b \ul Step 1 : Create hive table as said. 
\b0 \ulnone \
hive \
show tables; \
create table departments_hive(department_id int, department_name string); \

\b \ul Step 2 : The important here is, when we create a table without delimiter fields. Then default delimiter tor hive is ^A ( 001): 
\b0 \ulnone \
Hence, while importing data we have to provide proper delimiter. \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera  --table departments --hive-home /user/hive/warehouse --hive-import --hive-overwrite  --hive-table departments _ hive  --fields-terminated-by \'91 001\'92 \

\b \ul Step 3 : Check the data in directory. 
\b0 \ulnone \
hdfs dfs -ls /user/hive/warehouse/departments_hive \
hdfs dfs -cat /user/hive/warehouse/departments_hive/part* \
Check data in hive table. \
select * from departments_hive;\
------------------------------------------------------------------\

\b \ul Problem Scenario 23: 
\b0 \ulnone You have given a CSV file, which contain Employee and Salary data, You need to accomplish following\
\
1. Load this file to HDFS\
2. Create two tables in MySQ1name as  EMPLOYEE and SALARY.\
3. Once file in HDFS load in RDBMS using sqoop on above two tables.\
4. Create an Avro file for t his EMPLOYEE and SALARY tables.\
5. Extract the schema from this Avro file (And without downloading AVRO file locally).\
\

\b \ul Employee.csv
\b0 \ulnone \
1001,Amit,male,35\
1002,Lokesh,male,36\
1003,Venkat,male,28\
1004,Radha,female,30\
1005,Vanita,female,42\
\

\b \ul Salary.csv
\b0 \ulnone \
1001,120000\
1002,99000\
1003,106000\
1004,79000\
1005,89000\
----------------------------------------------------------------\

\b \ul Solution : 
\b0 \ulnone \
\

\b \ul Step 1 : Create a csv file on loca1System, named Employee.csv and Salary.csv (On Desktop under the folder CCA175).
\b0 \ulnone  \
Employee.csv : /home/cloudera/Desktop/CCA175/Employee.csv \
Salary.csv : /home/cloudera/Desktop/CCA175/Salary.csv \

\b \ul Step 2 : Create a Directory in HDFS, using the shell. 
\b0 \ulnone \
hdfs dfs -mkdir module_avro \

\b \ul Step 3 : Check whether Directory created or not 
\b0 \ulnone \
hdfs dfs -ls \

\b \ul Step 4 : Upload both the files to HDFS using put command. 
\b0 \ulnone \
hdfs dfs -put /home/cloudera/Desktop/CCA175/Employee.csv module_avro/ \
hdfs dfs -put /home/cloudera/Desktop/CCA175/Salary.csv module_avro/ \

\b \ul Step 5 : Check whether both the files have been copied or not. 
\b0 \ulnone \
hdfs dfs -Is module_avro \
hdfs dfs -cat module_avro/Employee.csv \
hdfs dfs -cat module_avro/Salary.csv \

\b \ul Step 6 : Create two tables in MySQ1do first.
\b0 \ulnone  \
mysq1--user=retail_dba --password=cloudera retail_db \
Empoyee Table : \
CREATE TABLE IF NOT EXISTS EMPLOYEE(id int, name char(100), sex char(5), age int j); \
Salary Table : \
CREATE TABLE IF NOT EXISTS SALARY(id int, salary int);\
Check table created or not . \
show tables;\

\b \ul Step 7 : Check the Help tor Sqoop export command
\b0 \ulnone . \
sqoop export --help \

\b \ul Step 8 : Once the tile is in HDFS, using Sqoop create tables as wel1as load the data in it. 
\b0 \ulnone \
sqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table EMPLOYEE --direct --export-dir module_avro/Employee.csv -m 1 \
sqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table SALARY --direct --export-dir module_avro/Salary.csv -m 1\

\b \ul Step 9 : Check whether data is loaded or not. 
\b0 \ulnone \
select * from EMPLOYEE; \
select * from SALARY;\

\b \ul Step 10 : using Sqoop Extract Employee and Salary and its data as an Avro file.
\b0 \ulnone  \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --as-avrodatafile --warehouse-dir module_avro --table EMPLOYEE --Spilt-by id -m 1\
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --as-avrodatafile --warehouse-dir module_avro --table SALARY --Spilt-by id -m 1\

\b \ul Step 11: Check whether avro tiles extracted or not.
\b0 \ulnone  \
hdfs dfs -lS module avrO/EMPLOYEE \
hdfs dfs -ls module avro/SALARY \
hdfs dfs -cat module_avro/EMPLOYEE/part-m-OOOOO.avro \
hdfs dfs -cat module_avro/SALARY/part-m-OOOOO.avro \

\b \ul Step 12 : Check al1the available Options for avro tools. 
\b0 \ulnone \
java -jar /usr/lib/avro/avro-tools.jar\
 
\b \ul Step 13 : using Avro too1extract the schema from Avro tiles. 
\b0 \ulnone \
java -jar /usr/lib/avro/avro-tools.jar getschema hdts://quickstart.cloudera:8020/user/cloudera/module_avro/EMPLOYEE/part-m-OOOOO.avro > Employee.avsc\
java -jar /usr/lib/avro/avro-tools.jar getschema hdts://quickstart.cloudera:8020/user/cloudera/module_avro/SALARY/part-m-OOOOO.avro > Salary.avsc\
------------------------------------------------------------------------\

\b \ul Problem Scenario 24:
\b0 \ulnone  you have been given following mysq1database details as wel1as other info\
user=retail_dba  password=cloudera database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_dba\
Please accomplish below assignment. \

\b \ul 1.Create a table in hive as below.
\b0 \ulnone \
create table departments_hive01(department_id int, department_name string, avg_salary int);\

\b \ul 2.Create another table in mysq1using below statement
\b0 \ulnone \
CREATE TABLE IF NOT EXISTS departments_hive01(id int, department_name varchar(45), avg_salary int);\

\b \ul 3.Copy al1the data from departments table to departments_hive01 using
\b0 \ulnone \
insert into departments_hive01 select a.*, nul1from departments a;\
Also inset following records as below\
insert into departments_hive01 values(777, "Notkn0wn", 1000);\
insert into departments_hive01 values(8888, null, 1000);\
insert into departments_hive01 values(666, null, 1100);\

\b \ul 4. Now Import data from mysq1table departments_hive01 to this hive table. Please make sure that data should be visible using below hive command.
\b0 \ulnone \
Also, while importing if nul1value found for department_name column replace it with "" (empty string) and for id columm with -999\
select * from departments_hive;\
------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 25
\b0 \ulnone : you have been given following mysq1database details as wel1as other info\
user=retail_dba password=cloudera database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Now accomplish following activities.\
\

\b \ul 1. Create mysq1table as below.
\b0 \ulnone \
mysq1--user=retail_dba --password=cloudera\
use retail_db;\
CREATE TABLE IF NOT EXISTS departments_hive02(id int, department_name varchar(45), avg_salary int);\
show tables;\

\b \ul 2.Now export data from hive table departments_hive01 in departments_hive02. while exporting, please note following.
\b0 \ulnone \
wherever there is a empty string it should be loaded as a null value in mysql\
wherever there is -999 value for int field, it should be created as null value.\
\
------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenario 24
\b0 \ulnone : you have been given following mysq1database details as wel1as other info\
\
user=retail_dba password=cloudera  database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
\
Please accomplish below assignment. \
\
1.Create a table in hive as below.\
create table departments_hive01(department_id int, department_name string, avg_salary int);\
2.Create another table in mysql using below statement\
CREATE TABLE IF NOT EXISTS departments_hive01(id int, department_name varchar(45), avg_salary int);\
3.Copy al1the data from departments table to departments_hive01 using\
insert into departments_hive01 select a.*, null from departments a;\
Also inset following records as below\
insert into departments_hive01 values(777, "Not known", 1000);\
insert into departments_hive01 values(8888, null, 1000);\
insert into departments_hive01 values(666, null, 1100);\
4. Now Import data from mysql table departments_hive01 to this hive table. Please make sure that data should be visible using below hive command.\
Also, while importing if null value found for department_name column replace it with "" (empty string) and for id column with -999\
select * from departments_hive;\
\

\b \ul Solution :
\b0 \ulnone  \
\

\b \ul Step 1 :
\b0 \ulnone  Create hive table as below. \
hive \
show tables; \
create table departments_hive0l(department_id int, department_name string, avg_salary int); \

\b \ul Step 2 :
\b0 \ulnone  Create table in mysql db as well. \
mysql-user=retail_dba --password=cloudera\
\'97\'97use retail_db;\
CREATE TABLE IF NOT EXISTS departments_hive01(id int, department_name varchar(45), avg_salary int); \
show tables; \

\b \ul Step 3 
\b0 \ulnone : Insert data in mysql table. \
insert into departments_hive01 select a.*, null from departments a; \
check data inserts select * from departments_hive01; \
Now inserts null records as given in problem. \
insert into departments_hive01 values(777, "Not known",1000) \
insert into departments_hive01 values(8888, null,1000); \
insert into departments_hive01 values(666, null,1100); \

\b \ul Step 4 : 
\b0 \ulnone Now import data in hive as per requirement. \
sqoop import   --connect jdbc:mysql://quickstart:3306/retail_db   --username=retail_dba   --password=cloudera   --table departments_hive01   --hive-home /user/hive/warehouse   --hive-import   --hive-overwrite   --hive-table departments_hive01   --fields-terminated-by ' OO1\'92  --null-string ""   --null-non-string -999   --split-by id   \

\b \ul Step 5 :
\b0 \ulnone  Check the data in directory. \
hdfs dfs -ls /user/hive/warehouse/departments_hive01\
hdfs dfs -cat luser/hive/warehouse/departments_hiveol/pare \
Check data in hive table. \
select * from departments_hive01; \
==================================================================================================================================\

\b \ul Problem Scenario 25:
\b0 \ulnone  you have been given following mysql database details as well as other info\
user=retail_dba password=cloudera database=retail_db jdbcURl= jdbc:mysql://quickstart:3306/retail_db\
Now accomplish following activities.\
1. Create mysql table as below.\
mysql\'97user=retail_dba --password=cloudera\
use retail_db\
CREATE TABLE IF NOT EXISTS departments_hive02(id int, department_name varchar(45), avg_salary int);\
show tables;\
2.Now export data from hive table departments_hive01 in departments_hive02. while exporting, please note following.\
wherever there is a empty string it should be loaded as a null value in mysql\
wherever there is -999 value for int field, it should be created as null value.\
\

\b \ul Solution : 
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create table in mysql db as well. \
mysql -user=retail_dba --password=cloudera \
use retail_db \
CREATE TABLE IF NOT EXISTS departments_hive02(id int, department_name varchar(45), avg_salary int); \
show tables; \

\b \ul Step 2 :
\b0 \ulnone  Now export data from hive table to mysql table as per the requirement. \
sqoop export --connect jdbc:mysql://quickstart:3306/retail_db   --username retail_dba   --password cloudera   --table departments_hive02   --export-dir /user/hive/warehouse/departments_hive01  --input-fields-terminated-by ' 001'   --input-lines-terminated-by ' n'   --num-mappers 1   --batch   --input-null-string ""   --input-null-non-string -999 \

\b \ul Step 3 : \ulnone Now validate the data.\ul  
\b0 \ulnone \
select * from departments_hive02; \
=================================================================================================================================\

\b \ul Problem Scenario 26
\b0 \ulnone : you have been given following mysql database details as well as other info\
user=retail_dba password=cloudera  database=retail_db jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Now accomplish following activities.\
1.Import departments table from mysql to hdfs as textfile in deppartments_text directory.\
2.Import departments table from mysql to hdfs as sequencefile in departments_sequence directory.\
3.Import departments table from mysql to hdfs as avro file in departments_avro directory\
4.Import departments table from mysql to hdfs as parquet file in departments_parquet directory.\
\

\b \ul Solution : 
\b0 \ulnone \

\b \ul Step 1 
\b0 \ulnone : Import departments table from mysq1to hdfs as texttile \
sqoop import   --connect jdbc:mysql://quickstart:3306/retail_db   --username=retail_dba   --password=cloudera   --table departments   --as-textfile   \
verify imported data \
hdfs dfs -cat departments_text/part* \

\b \ul Step 2 :
\b0 \ulnone  Import departments table from mysql to hdfs as sequencefile \
sqoop import   --connect jdbc:mysql://quickstart:3306/retail_db   --username=retail_dba   --password=cloudera   --table departments   --as-sequencefile  --target-dir=departments_sequence \
verify imported data \
hdfs dfs -cat departments_sequence/part* \

\b \ul Step 3 :
\b0 \ulnone  Import departments table from mysql to hdfs as sequencefile \
sqoop import   --connect jdbc:mysql://quickstart:3306/retail_db   --username=retail_dba   --password=cloudera   --table departments   --as-avrodatafile   --target-dir=departments_avro \
verify imported data \
hdfs dfs -cat departments_avro/part* \

\b \ul Step 4 :
\b0 \ulnone  Import departments table from mysql to hdfs as sequencefile \
sqoop import   --connect jdbc:mysql://quickstart:3306/retail_db   --username=retail_dba   --password=cloudera   --table departments   --as-parquetfile   --target-dir=depsrtments_parquet\
verify imported data \
hdfs dfs -cat departments_parquet/part*\
==================================================================================================================================\

\b \ul Problem Scenario 27:
\b0 \ulnone  you have been given following mysql database details as well as other info\
\
user=retail_dba password=cloudera database=retail_db jdbcURL= jdbc:mysql://quickstart:3306/retail_db\
Please accomplish following activities.\
1.write a Sqoop job which will import "retail_db.categories" table to hdfs, in a directory name "categories_target_job".\

\b \ul Solution : 
\b0 \ulnone \

\b \ul Step 1 : 
\b0 \ulnone Connecting to existing MySQ1Database \
mysql\'97user=retail_dba --password=cloudera retail_db \

\b \ul Step 2 : 
\b0 \ulnone Show all the available tables \
show tables; \

\b \ul Step 3 :
\b0 \ulnone  Below is the command to create Sqoop Job (Please note that \'97 import space is mandatory) \
sqoop job --create sqoop_job   --import   --connect "jdbc:mysql://quickstart:3306/retail_db"   --username=retail_dba   --password=cloudera   --table categories   --target-dir categories_target_job   --fields-terminated-by '|'   --lines-terminated-by ' n'\

\b \ul Step 4 : 
\b0 \ulnone List al1the Sqoop Jobs \
sqoop job --list \

\b \ul Step 5 :
\b0 \ulnone  Show details of the Sqoop Job \
sqoop job --show sqoop_job \

\b \ul Step 6 : 
\b0 \ulnone Execute the sqoop job \
sqoop job --exec sqoop_job \

\b \ul Step 7 : 
\b0 \ulnone Check the output of import job \
hdfs dfs -ls categories_target_job \
hdfs dfs -cat categories_target_job/part* \
\
==================================================================================================================================\
Problem Scenario 28: you have been given log generating service as below.\
\
Start_logs (It wil1generate continuous logs)\
tail_logs (you can check, what logs are being generated)\
stop_logs (it wil1stop the log service)\
Path where logs are generated using above service : /opt/gen_logs/logs/access.log\
Now write a flume configuration file named flume1.conf, using that configuration file dumps logs in HDFS file system in a directory called  flume1.\
Flume channe1should have following property as well. After every 100 message it should be committed, use non-durable/faster channe1and it should be able to hold maximum 1000 events.\
\

\b \ul Solution : 
\b0 \ulnone \
\

\b \ul Step 1 : Create flume configuration file, with below configuration for source, sink and channe1
\b0 \ulnone \
#Define source , sink , channe1and agent. \
agent1.sources = source1\
agent1.sinks = sink1\
agent1.channels = channel1\
# Describe/configure source1\
agent1.sources.source1.type = exec \
agent1.sources.source1.command = tail -F /opt/gen_logs/logs/access.log \
## Describe sink1\
agent1.sinks.sink1.channe1= memory-channel\
agent1.sinks.sink1.type = hdfs \
agent1.sinks.sink1.hdfs.path = flume1\
agent1.sinks.sink1.hdfs.fileType = DataStream \
# Now we need to define channel1 property. \
agent1.channels.channel1.type = memory \
agent1.channels.channel1.capacity = 1000 \
agent1.channels.channel1.transactioncapacity=100 \
# Bind the source and sink to the channe1\
agent1.sources.source1.channels = channel1 \
agent1.sinks.sink1.channel= channel1  \

\b \ul Step 2 : Run below command which wil1use this configuration file and append data in hdfs. 
\b0 \ulnone \
Start log service using : start_logs \
Start flume service : \
flume-ng agent --conf /home/cloudera/flumeconf --conf-file /home/cloudera/flumeconf/flume1.conf -Dflume.root.logger=DEBUG,lNFO,console --name agent1 \
Wait for few mins and than stop log service. \
stop_logs \
=============================================================================================================================\

\b \ul Problem Scenario 29: 
\b0 \ulnone You have been given below comma separated employee information.\
\
name,salary,sex,age\
alok,100000,male,29\
jatin,105000,male,32\
yogesh,134000,male,39\
ragini,112000,female,35\
jyotsana,129000,female,39\
valmiki,123000,male,29\
Use the netcat service on port 44444, and nc above data line by line. Please do the following activities.\
1.Create a flume conf file using fastest channel, which write data in hive warehouse directory, in a table called flumeemplyee ( Create hive table as well for given data).\
2. Write a hive query to read average salary of al1employees.\
\

\b \ul solution : 
\b0 \ulnone \
\

\b \ul Step 1 : Create hive table for flumeemployee.
\b0 \ulnone \
CREATE TABLE flumeemployee \
(\
name string, \
salary int, \
sex string, \
age int \
) ROW FORMAT DELIMITED FIELDS TERMINATED BY  ','; \

\b \ul Step 2 : Create flume configuration file, with below configuration for source, sink and channe1and save it in flume2.conf.
\b0 \ulnone  \
#Define source , sink , channe1and agent. \
agent1.sources = source1\
agent1.sinks = sink1\
agent1.channels = channel1\
# Describe/contigure source1\
agent1.sources.source1.type = netcat \
agent1.sources.source1.bind = 127.0.0.1 \
agent1.sources. source1.port = 44444 \
## Describe sink1\
agent1.sinks.sink1.channe1= memory-channe1\
agent1.sinks.sink1.type = hdfs \
agent1.sinks. sink1.hdfs.path = /user/hive/warehouse/flumeemployee \
agent1.sinks.hdfs-write.hdfs.writeFormat=Text\
agent1.sinks.sink1.hdfs.fileType = DataStream \
# Now we need to define channel1 property. \
agent1.channels.channel1.type = memory \
agent1.channels.channel1.capacity = 1000 \
agent1.channels.channel1.transactioncapacity =100\
# Bind the source and sink to the channe1\
agent1.sources.source1.channels = channel1\
agent1.sinks.sink1.channe1= channel1\
\

\b \ul Corrected :- 
\b0 \ulnone \
\
#Source , sink , channe1and agent. \
agent1.sources = source1\
agent1.sinks = sink1\
agent1.channels = channel1\
# Describe/contigure source1\
agent1.sources.source1.type = netcat \
agent1.sources.source1.bind = 127.0.0.1 \
agent1.sources.source1.port = 44444 \
## Describe sink1\
agent1.sinks.sink1.channe1= memory-channe1\
agent1.sinks.sink1.type = hdfs \
agent1.sinks.sink1.hdfs.path = /user/hive/warehouse/flumeemployee/\
agent1.sinks.sink1.HDFS.hdfs.writeFormat = Text\
agent1.sinks.sink1.HDFS.hdfs.fileType = DataStream \
# Now we need to define channel1 property. \
agent1.channels.channel1.type = memory \
agent1.channels.channel1.capacity = 1000 \
agent1.channels.channel1.transactioncapacity =100\
# Bind the source and sink to the channe1\
agent1.sources.source1.channels = channel1\
agent1.sinks.sink1.channel= channel1\
\
 
\b \ul Step 3 : Run below command which wil1use this configuration file and append data in hdfs
\b0 \ulnone . \
Start flume service : \
flume-ng agent --conf /home/cloudera/flumeconf \'97conf-file /home/cloudera/flumecont/flume2.conf \'97name agent1\

\b \ul Step 4 : Open another termina1and use the netcat service.
\b0 \ulnone  \
nc localhost 44444 \
S
\b \ul tep 5 : Enter data line by line.
\b0 \ulnone \
alok,100000,male,29\
jatin,105000,male,32 \
yogesh,134000,male,39 \
ragini,112000,female,35 \
jyotsana,129000,female,39 \
S
\b \ul tep 6 : Open hue and check the data is available in hive table or not.
\b0 \ulnone  \

\b \ul Step 7 : Stop flume service by pressing ctrl+c
\b0 \ulnone  \

\b \ul Step 8 : Calculate average salary on hive table using below query. You can use either hive command line too1or hue
\b0 \ulnone . \
select avg(salary) from flumeemployee; \
=============================================================================================================================\

\b \ul Problem Scenario 30: 
\b0 \ulnone You have been given log generating service as below\
Start_logs (It will generate continuous logs)\
tail_logs (You can check, what logs are being generated)\
stop_logs (it wil1stop the log service)\
Path where logs are generated using above service : /opt/gen_logs/logs/access.log\
Now write a flume configuration file named flume3.conf, using that configuration file dumps logs in HDFS file system in a directory called flume3%Y/%m/%d/%H/%M\
(Means every minute new directory should be created). Please us the interceptors  to provide timestamp information, if message header does not have header info.\
And also note that you have to preserve existing timestamp, if message contains it.\
Flume channel should have following property as well. After every 100 message it should be committed, use non-durable/faster channe1 and it should be able to hold maximum 1000 events.\

\b \ul Solution : 
\b0 \ulnone \

\b \ul Step 1 : Create flume configuration file, with below configuration for source, sink and channel. 
\b0 \ulnone \
#Define source , sink , channe1and agent. \
agent1.sources = source1\
agent1.sinks = sink1\
agent1.channels = channel1\
# Describe/configure source1\
agent1.sources.source1.type = exec \
agent1.sources.source1.command = tail- F /opt/gen_logs/logs/access.log \
#Define interceptors \
agentl.sources.source1.interceptors=i1 \
agentl.sources.source1.interceptors.i1.type=timestamp \
agentl.sources.source1.interceptors.i1.preserveExisting\'97true \
## Describe sink1\
agent1.sinks.sink1.channe1= memory-channe1\
agent1.sinks.sink1.type = hdfs \
agent1.sinks.sink1.hdfs.path = flume3/%Y/%m/%d/%H/%M \
agent1.sinks.sink1.hdfs.fileType = DataStream \
# Now we need to define channel1 property. \
agent1.channels.channel1.type = memory \
agent1.channels.channel1.capacity = 1000 \
agent1.channels.channel1.transactioncapacity = 100\
# Bind the source and sink to the channe1\
agent1.sources.source1.channels = channel1 \
agent1.sinks.sink1.channel= channel1\
 
\b \ul Step 2 : Run below command which wil1use this configuration tile and append data in hdfs
\b0 \ulnone . \
Start log service using : start_logs \
Start flume service : \
flume-ng agent --conf /home/cloudera/flumeconf \'97conf-file /home/cloudera/flumeconf/flume3.conf -Dflume.root.logger=DEBUG,INFO,console --name agent1\
 Wait tor few mins and than stop log service. \
stop_logs \
================================================================================================================================\

\b \ul Problem Scenario 31
\b0 \ulnone : You have been given below comma separated employee information.\
name,salary,sex,age\
alok,100000,male,29\
jatin,105000,male,32\
yogesh,134000,male,39\
ragini,112000,female,35\
jyotsana,129000,female,39\
valmiki,123000,male,29\
Use the netcat service on port 44444, and nc above data line by line. Please do the following activities.\
1.Create a flume conf file using fastest channel, which write data in hive warehouse directory, in a table called flumeemplyee ( Create hive table as wel1for given data).\
2. While importing, make sure only male employee data is stored.\
\

\b \ul Solution : 
\b0 \ulnone \

\b \ul Step 1 : Create hive table tor flumeemployee.
\b0 \ulnone \
CREATE TABLE flumemaleemployee \
(\
name string, \
salary int, \
sex string, \
age int \
)\
ROW FORMAT DELIMITED \
FIELDS TERMINATED BY ',';\

\b \ul Step 2 : Create flume configuration file, with below configuration for source, sink and channe1and save it in flume4.conf
\b0 \ulnone . \
#Define source , sink , channe1and agent. \
agent1.sources = source1\
agent1.sinks = sink1\
agent1.channels = channel1\
# Describe/contigure source1\
agent1.sources.source1.type = netcat \
agent1.sources.source1.bind = 127.0.0.1 \
agentl. sources. source1.port = 44444 \
#Define interceptors \
agentl. sources. sourcel. interceptors=i1\
agentl. sources. source 1. interceptors. i1.type=regex_filter \
agentl. sources. sourcel. interceptors. il. regex=female \
agentl. sources. sourcel. interceptors. i1.excludeEvents=true \
## Describe sink1\
agent1.sinks.sink1.channe1= memory-channe1\
agent1.sinks.sink1.type = hdfs \
agentl. sinks. sink1.hdfs.path = /user/hive/warehouse/flumemaleemployee \
hdfs-agent.sinks.hdfs-write.hdfs.writeFormat=Text \
agent1.sinks.sink1.hdfs.fileType = DataStream \
# Now we need to define channel1property. \
agent1.channels.channel1.type = memory \
agent1.channels.channel1.capacity = 1000 \
agent1.channels.channell. transactioncapacity=100\
# Bind the source and sink to the channe1\
agent1.sources.source1.channels = channel1 \
agent1.sinks.sink1.channe1= channel1\

\b \ul Step 3 : Run below command which wil1use this configuration file and append data in hdfs
\b0 \ulnone . \
Start flume service : \
flume-ng agent --conf /home/cloudera/flumeconf --conf-file /home/cloudera/flumeconf/flume4.conf --name agent1\

\b \ul Step 4 : Open another termina1and use the netcat service.
\b0 \ulnone  \
nc localhost 44444 \

\b \ul Step 5 : Enter data line by line.
\b0 \ulnone  \
alok,100000,male,29\
jatin,105000,male,32\
yogesh,134000,male,39 \
ragini,112000,temale,35 \
jyotsana,129000,temale,39 \

\b \ul Step 6 : Open hue and check the data is available in hive table or not.
\b0 \ulnone  \

\b \ul Step 7 : Stop flume service by pressing ctrl+c
\b0 \ulnone  \

\b \ul Step 8 : Calculate average salary on hive table using below query. You can use either hive command line too1or hue.
\b0 \ulnone  \
select avg(salary) from flumeemployee; \
================================================================================================================================\

\b \ul Problem Scenario 32:
\b0 \ulnone  You have been given below comma separated employee information. That need to be added in /home/cloudera/flumetest/in.txt file (to do tai1source)\
\
sex,name,city\
1,alok,mumbai\
1,jatin,chennai\
1,yogesh,kolkata\
2,ragini,delhi\
2,jyotsana,pune\
1,valmiki,banglore\
Create a flume conf file using fastest channe1 non-durable channel, which write data in hive warehouse directory, in two separate tables called flumemaleemployee1 and flumefemaleemployee1\
(Create hive table as wel1for given data). Please use tai1source with /home/cloudera/flumetest/in.txt file.\
\
flumemaleemployee1 : will contain only male employees data\
flumefemalemployee1 : will contain only woman employees data\
\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : 
\b0 \ulnone Create hive table for flumemaleemployee1and .' \
CREATE TABLE flumemaleemployee1\
(\
sex_type int , \
name string, \
city string\
) \
ROW FORMAT DELIMITED \
FIELDS TERMINATED BY ',';\
\
CREATE TABLE flumefemaleemployee1\
(\
sex type int , \
name string, \
city string \
)\
ROW FORMAT DELIMITED \
FIELDS TERMINATED BY','; \

\b \ul Step 2 :
\b0 \ulnone  Create below directory and file \
mkdir /home/cloudera/flumetest/ \
cd /home/cloudera/flumetest/ \

\b \ul Step 3 : 
\b0 \ulnone Create flume configuration file, with below configuration for source, sink and channel  and save it in flume5.conf. \
agent.sources = tailsrc \
agent.channels = mem1mem2 \
agent.sinks = std1std2 \
agent.sources.tailsrc.type = exec \
agent.sources.tailsrc.command = tai1-F /home/cloudera/flumetest/in.txt \
agent.sources.tailsrc.batchSize = 1 \
\
agent.sources.tailsrc.interceptors = i1\
agent.sources.tailsrc.interceptors.i1 .type = regex_extractor \
agent.sources.tailsrc.interceptors.i1 .regex =^(  d) \
agent.sources.tailsrc.interceptors.i1 .serializers = t1\
agent. sources.tailsrc.interceptors.i1 .serializers.t1.name = type \
\
agent.sources.tailsrc.selector.type = multiplexing \
agent.sources.tailsrc.selector.header = type \
agent.sources.tailsrc.selector.mapping.1 = mem1\
agent. sources.tailsrc.selector.mapping.2 = mem2 \
\
agent.sinks.std1.type = hdfs \
agent.sinks.std1.channe1= mem1\
agent.sinks.std1.batchSize = 1 \
agent.sinks.std1.hdfs.path = /user/hive/warehouse/flumemaleemployee1 \
agent.sinks.std1.rolllnterva1= O \
agent.sinks.std1.hdfs.fileType = DataStream \
\
agent.sinks.std2.type = hdfs \
agent.sinks.std2.channe1= mem2 \
agent.sinks.std2.batchSize = 1 \
agent.sinks.std2.hdfs.path = /user/hive/warehouse/flumefemaleemployee1\
agent.sinks.std2.rolllnterva1= 0\
agent.sinks.std2.hdfs.fileType = DataStream \
\
agent.channels.mem1.type = memory \
agent.channels.mem1.capacity = 100 \
agent.channels.mem2.type = memory \
agent.channels.mem2.capacity = 100 \
agent.sources.tailsrc.channels = mem1mem2 \
\

\b \ul Step 4 :
\b0 \ulnone  Run below command which will use this configuration file and append data in hdfs. \
Start flume service : \
flume-ng agent \'97conf /home/cloudera/flumeconf --conf-file /home/cloudera/flumeconf/flume5.conf --name agent \
\

\b \ul Step 5 :
\b0 \ulnone  Open another terminal create a file at /home/cloudera/flumetest/in.txt . \
\

\b \ul Step 6 :
\b0 \ulnone  Enter below data in file and save it. \
l,alok,mumbai \
l,jatin,chennai \
l,yogesh,kolkata \
2,ragini,delhi \
2,jyotsana,pune \
l,valmiki,banglore\
Step 7:Open hue and check the data is available in hive table or not\
Step 8:Stop flume service by pressing ctrl+c \
\
==================================================================================================================================\
\

\b \ul Problem Scenario 33: 
\b0 \ulnone you need to implement near real time solutions for collecting information when submitted in file with below information.\
\
You have been given below directory location (if not available than create it ) /tmp/nrtcontent. Assume your departments upstream service is continuously committing data in this directory as a new file ( not stream of data, because it is near real time solution). As soon as file committed in this directory that needs to be available in hdfs in /tmp/flume location.\
\
Write a flume configuration file named flume6.conf and use it to load data in hdfs with  following additional properties.\
\
1. Spoo1/tmp/nrtcontent\
2. File prefix in hdfs should be events\
3. File suffix should be .log\
4. if file is not committed and in use than it should have_as prefix.\
5.Data should be written as text to hdfs\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create directory \
mkdir /tmp/nrtcontent \

\b \ul step 2 :
\b0 \ulnone Create flume configuration file, with below configuration for source, sink and channel and save it in flume6.conf. \
agent1.sources = source1\
agent1.sinks = sink1\
agent1.channels = channel1 \
\
agent1.sources.source1.channels = channel1 \
agent1.sinks.sink1.channe1= channel1 \
\
agent1.sources.source1.type = spooldir \
agent1. sources. source1. spoolDir = /tmp/nrtcontent \
\
agent1.sinks.sink1.type = hdfs \
agent1.sinks.sink1.hdfs.path = /tmp/flume \
agent1.sinks.sink1.hdfs.filepretix = events \
agent1.sinks.sink1.hdfs.fileSuffix = .log \
agentl. sinks. sink1.hdts.inusepretix =_\
agent1.sinks.sink1.hdfs.fileType = DataStream \
\
agent1.channels.channel1 .type = file \
\

\b \ul Step4: 
\b0 \ulnone Run below command which will use this configuration file and append data in hdfs. \
Start flume service : \
flume-ng agent --conf /home/cloudera/flumeconf --conf-file /home/cloudera/flumeconf/flume6.conf --name agent1\

\b \ul Step 5 
\b0 \ulnone : Open another terminal and create a file in /tmp/nrtcontent \
echo "i am preparing tor CCA175 from HadoopThoughtworks.com" >/tmp/nrtcontent/.he1.txt \
mv /tmp/nrtcontent/.he1.txt /tmp/nrtcontent/he1.txt \
After few mins \
echo "1am preparing tor CCA175 trom HadoopThoughtworks.com" > /tmp/nrtcontent/.qt1.txt \
mv /tmp/nrtcontent/.qt1.txt /tmp/nrtcontent/qt1.txt \
\
===================================================================================================================================\

\b \ul Problem Scenario 34:
\b0 \ulnone  You need to implement near real time solutions for collecting information when submitted in file with below information.\
\
You have been given below directory location ( If not available than create it ) /tmp/spooldir). you have a financial subscription for getting stock prices from BloomBerg as well as Reuters and using ftp you download every hour new file from their respective ftp site in directories /tmp/spooldir/bb/ and /tmp/spooldir/dr respectively.\
As soon as file committed in this directory that needs to be available in hdfs in /tmp/flume/finance location in a single directory.\
Write a flume configuration file named flume7.conf and use it to load data in hdfs with following additional properties.\
\
1. Spool/tmp/spooldir/bb and /tmp/spooldir/dr\
2. File prefix in hdfs should be events\
3. File suffix should be .log\
4. if file is not committed and in use than it should have _ as prefix.\
5. Data should be written as text to hdfs\
--------------------------------------------------------------------------------------------------------------------------\
Solution:\

\b \ul Step 1 : 
\b0 \ulnone Create directory \
mkdir /tmp/spooldir/bb \
mkdir /tmp/spooldir/dr \

\b \ul step 2 
\b0 \ulnone :Create flume configuration file, with below configuration for source, sink and channel and save it in flume7.conf. \
\
agentl.sources = source1source2 \
agent1.sinks = sink1\
agent1.channels = channel1 \
\
agent1.sources.source1.channels = channel1 \
agent1.sources.source2.channels = channel1 \
agent1.sinks.sink1.channe1= channel1 \
\
agent1.sources.source1.type = spooldir \
agent1.sources.source1.spoolDir = /tmp/spooldir/bb \
agent1.sources.source2.type = spooldir \
agent1. sources. source2. spoolDir = /tmp/spooldir/dr \
\
agent1.sinks.sink1.type = hdfs \
agent1.sinks.sink1.hdfs.path = /tmp/flume/finance \
agent1.sinks.sink1.hdfs.filepretix = events \
agent1.sinks:sink1.hdfs.fileSuffix = .log \
agent1.sinks.sink1.hdfs.inusepretix =_ \
agent1.sinks.sink1.hdfs.fileType = DataStream \
agent1.channels.channel1.type = file \
\

\b \ul Step 4 :
\b0 \ulnone  Run below command which will use this configuration file and append data in hdfs. \
Start flume service : \
flume-ng agent --conf /home/cloudera/flumeconf --conf-file /home/cloudera/flumecont/flume7.conf --name agent1\

\b \ul Step 5 :
\b0 \ulnone  Open another terminal and create a file in /tmp/spooldir/ \
echo "IBM,100,20160104">> /tmp/spooldir/bb/.bb.txt \
echo "IBM,103,20160105">> /tmp/spooldir/bb/.bb.txt \
mv /tmp/spooldir/bb/.bb.txt /tmp/spooldir/bb/bb.txt \
After few mins \
echo "IBM,100.2,20160104">> /tmp/spooldir/dr/.dr.txt \
echo "IBM, 103.1 ,20160105" >> /tmp/spooldir/dr/.dr.txt \
mv /tmp/spooldir/dr/.dr.txt /tmp/spooldir/dr/dr.txt \
-------------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 35:
\b0 \ulnone  you need to implement near real time solutions for collecting information when submitted in file with below information.\
You have been given below directory location ( If not available than create it ) /tmp/spooldir2.\
As soon as file committed in this directory that needs to be available in hdfs in /tmp/flume/primary as wel1as /tmp/flume/secondary location.\
However, note that /tmp/flume/secondary is optional, if transaction failed which writes in this directory need not to be rollback.\
\
Write a flume configuration file named flume8.conf and use it to load data in hdfs with following additional properties.\
1. Spoo1/tmp/Spooldir2 Directory\
2. File Prefix in hdfs should be events.\
3. File suffix should bd .log\
4. if file is not commited and in use than it should have _ as perfix.\
5. Data should be written as text to hdfs.\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create directory \
mkdir /tmp/spooldir2 \

\b \ul step 2 :
\b0 \ulnone Create flume configuration file, with below configuration for source, sink and channel and save it in flume8.conf. \
\
agentl.sources = source1\
agent1.sinks = sink1a sink1b \
agent1.channels = channel1a channel1b \
\
agent1.sources.source1.channels = channel1a channel1b \
agent1.sources.source.selector.type = replicating\
agent1.sources.source1.selector.optional=channel1b \
agent1.sinks.sinkla .channe1= channel1a \
agent1.sinks.sinklb .channe1= channel1b\
\
agent1.sources.source1.type = spooldir \
agent1.sources.source1.spoolDir = /tmp/spooldir2 \
\
\
agent1.sinks.sinkla .type = hdfs \
agent1.sinks.sinkla .hdfs.path = /tmp/flume/primary \
agent1.sinks.sinkla .hdfs.filepretix = events \
agent1.sinks.sinkla .hdfs.fileSuffix = .log \
agent1.sinks.sinkla .hdfs.fileType = DataStream \
\
agent1.sinks.sinklb .type = hdfs \
agent1.sinks.sinklb .hdfs.path = /tmp/flume/secondary\
agent1.sinks.sinklb .hdfs.filepretix = events \
agent1.sinks.sinklb .hdfs.fileSuffix = .log \
agent1.sinks.sinklb .hdfs.fileType = DataStream \
\
agent1.channels.channela .type = file \
agent1.channels.channelb .type = memory\
\

\b \ul Step 4 : 
\b0 \ulnone Run below command which will use this configuration file and append data in hdfs. \
Start flume service : \
flume-ng agent --conf /home/cloudera/flumeconf --conf-file /home/cloudera/flumecont/flume8.conf --name agent1\

\b \ul Step 5 :
\b0 \ulnone  Open another terminal and create a file in /tmp/spooldir2/ \
echo "IBM,100,20160104">> /tmp/spooldir2/.bb.txt \
echo "IBM,103,20160105">> /tmp/spooldir2/.bb.txt \
mv /tmp/spooldir2/.bb.txt /tmp/spooldir2/bb.txt \
After few mins \
echo "IBM,100.2,20160104">> /tmp/spooldir2/.dr.txt \
echo "IBM, 103.1 ,20160105" >> /tmp/spooldir2/.dr.txt \
mv /tmp/spooldir2/.dr.txt /tmp/spooldir2/dr.txt \
\
=============================================================================================================================\
\

\b \ul Problem Scenario 36
\b0 \ulnone : Please accomplish the following exercises using HDFS command line options.\
\
1. Create a directory in hdfs named hdfs_commands.\
2. Create a file in hdfs named data.txt in hdfs_commands.\
3. Now copy this data.txt file on local file system, however while copying file please make sure file properties are not changed e.g. file permissions.\
4. Now create a file in loca1directory named data_local.txt and move this file to hdfs in hdfs_commands directory\
5. Create a file data_hdfs.txt in hdfs_commands directory and copy it to loca1file system.\
6. Create a file in loca1filesystem named file1.txt and put it to hdfs\
\

\b \ul Solution :- 
\b0 \ulnone \
\

\b \ul Step 1 : 
\b0 \ulnone Create directory \
hdfs dfs -mkdir hdfs_commands \

\b \ul Step 2 : 
\b0 \ulnone Create a file in hdfs named data.txt in hdfs commands. \
hdfs dfs -touchz hdfs commands/data.txt \

\b \ul Step 3 :
\b0 \ulnone  Now copy this data.txt file on loca1filesystem, however while copying file please make sure file properties are not changed e.g.file permissions \
hdfs dfs -copyToLoca1-p hdfs_commands/data.txt lhome/cloudera/Desktop/HadoopThoughtworks \

\b \ul Step 4 :
\b0 \ulnone  Now create a file in loca1directory named data_local.txt and move this file to hdfs in hdfs commands directory. \
touch data_local.txt \
hdfs dfs -moveFromLoca1/home/cloudera/Desktop/HadoopThoughtworks/data_local.txt hdfs_commands/ \

\b \ul Step 5 :
\b0 \ulnone  Create a file data_hdfs.txt in hdfs commands directory and copy it to loca1file system. \
hdfs dfs -touch hdts commands/data hdts.txt \
hdfs dfs -get hdfs_commands/data_hdfs.txt /home/cloudera/Desktop/HadoopThoughtworks/ \

\b \ul Step 6 :
\b0 \ulnone  Create a file in loca1filesystem named file1.txt and put it to hdfs \
touch filel.txt \
hdfs dfs -put /home/cloudera/Desktop/HadoopThoughtworks/tile1 .txt hdfs_commands/ \
\
=================================================================================================================================\

\b \ul Problem Scenario 37:
\b0 \ulnone  you have been given three csv files in hdfs as below.\
EmployeeName.csv with the field ( id, name)\
EmployeeManager.csv (id, managerName)\
EmployeeSalary.cscv (id, Salary)\
EmployeeManager.csv\
E01,Vishnu\
E02,Satyam\
E03,Shiv\
E04,Sundar\
E05,John\
E06,Pallavi\
E07,Tanvir\
E08,Shekhar\
E09,Vinod\
E10,Jitendra\
EmployeeName.csv\
E01,Lokesh\
E02,Bhupesh\
E03,Amit\
E04,Ratan\
E05,Dinesh\
E06,Pavan\
E07,Tejas\
E08,Sheela\
E09,Kumar\
E10,Venkat\
EmployeeSalary.csv\
E01,50000\
E02,50000\
E03,45000\
E04,45000\
E05,50000\
E06,45000\
E07,50000\
E08,10000\
E09,10000\
E10,10000\
Using Spark and its API you have to generate a joined output as below and save as a text file (Separated by comma) for fina1distribution and output must be sorty by id.\
Id,name,salary,managerName\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create all three files in hdfs in directory called spark1(We will do using Hue). However, you can first create in local filesystem and then upload it to hdfs.\

\b \ul  Step 2 : 
\b0 \ulnone Load EmployeeManager.csv file from hdfs and create PairRDDs \
valmanager = sc.textFile("spark1/EmployeeManager.csv") \
valmanagerPairRDD = manager.map(x=> (x.split(",")(0),x.split(",")(1)))) \

\b \ul Step 3 :
\b0 \ulnone  Load EmployeeName.csv file from hdfs and create PairRDDs \
valname = sc.textFile("spark1/EmployeeName.csv") \
valnamePairRDD = name.map(x=> (x.split(",")(O),x.split(",")(1))) \

\b \ul Step 4 : 
\b0 \ulnone Load Employeesalary.csv file from hdfs and create PairRDDs \
valsalary = sc.textFile("spark1/EmployeeSalary.csv") \
valsalaryPairRDD = salary.map(x=> (x.split(",")(O),x.split(",")(1)))) \

\b \ul step 4 : 
\b0 \ulnone Join allpairRDDS \
valjoined = namePairRDD.join(salaryPairRDD).join(managerPairRDD) \

\b \ul Step 5 :
\b0 \ulnone  Now sort the joined results. \
valjoinedData = joined.sortByKey() \

\b \ul Step 6 :
\b0 \ulnone  Now generate comma separated data. \
valfinalData = joinedData.map(v=> (v._1,v._2._1._1,v._2._1._2,v._2._2)) \

\b \ul Step 7 :
\b0 \ulnone  Save this output in hdfs as text file. \
finalData.saveAsTextFile("spark1/result.txt") \
=================================================================================================================================\

\b \ul Problem Scenario 38
\b0 \ulnone : You have given following two files,\
1. Content.txt : Contain a huge text file containing space separated words.\
Hello this is HadoopThoughtworks.com \
This is HadoopThoughtworks.com\
Apache Spark Training\
This is Spark Learning Session\
2.Remove.txt : Ignore/filter al1the words given in this file (Comma Separated).\
Hello, is, this, the\
Write a Spark program which reads the content.txt and load as an RDD, Remove al1the words from a broadcast variables (Which is loaded as an RDD of words from Remove.txt). And count the occurrentce of the each word  and save it as a text file in HDFS.\
Spark is faster than MapReduce\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create al1three files in hdfs in directory called spark2 (We wil1do using Hue). However, you can first create in loca1filesystem and then upload it to hdfs.\

\b \ul step 2 :
\b0 \ulnone  Load the Content.txt file \
valcontent = sc.textFile("spark2/Content.txt") //Load the text file \

\b \ul Step 3 :
\b0 \ulnone  Load the Remove.txt file \
valremove = sc.textFile("spark2/Remove.txt") //Load the text tile \

\b \ul Step 4 :
\b0 \ulnone  Create an RDD from remove, However, there is a possibility each word could have trailing spaces, remove those whitespaces as well.We have used two functions here flatMap,map and trim\
valremoveRDD= remove.flatMap(x=> x.split(",") ).map(word=>word.trim)//Create an array of words \

\b \ul Step 5 :
\b0 \ulnone  Broadcast the variable, which you want to ignore \
valbRemove = sc.broadcast(removeRDD.collect().toList) // It should be array of Strings \

\b \ul Step 6 : 
\b0 \ulnone Split the content ROD, so we can have Array of String. \
valwords = content.flatMap(line => line.split(" ")) \

\b \ul Step 7 : 
\b0 \ulnone Filter the RDD, so it can have only content which are not present in "Broadcast Variable". \
valfiltered = words.filter\{case (word) => !bRemove.value.contains(word)\} \

\b \ul Step 8 : 
\b0 \ulnone Create a PairRDD, so we can have (word,l) tuple or PairRDD. \
valpairRDD = filtered.map(word => (word,l)) \

\b \ul Step 9 :
\b0 \ulnone  Now do the word count on PairRDD. \
valwordCount = pairRDD.reduceByKey(_ + _) \

\b \ul Step 10 
\b0 \ulnone : Save the output as a Text file. \
wordCount.saveAsTextFile("spark2/result.txt") \
=====================================================================================================================================\

\b \ul Problem Scenario 39 
\b0 \ulnone : You have given three file as below.\
spark3/sparkdir1/file1.txt\
Apache Hadoop is an open-source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. Al1the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework\
spark3/sparkdir2/file2.txt\
The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File System (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers packaged code for nodes to process in parallel based on the data that needs to be processed.\
spark3/sparkdir3/file3.txt\
This approach takes advantage of data locality nodes manipulating the data they have access to to allow the dataset to be processed faster and more efficiently than it would be in a more conventiona1supercomputer architecture that relies on a paralle1file system where computation and data are distributed via high-speed networking\
Each file contain some text.\
Now write a Spark code in scala which will load al1these three files from hdfs and do the word count by filtering following words.\
And result should be sorted by word count in reverse order.\
Filter words ("a","the","an","as","a","with","this","these","is","are","in","for","to","and","The","of")\
Also please make sure you load al1three files as a Single RDD ( Al1three files must be loaded using single API call).\
You have also been given following codec\
import org.apache.hadoop.io.compress.GzipCodec\
Please use above codec to compress file, while saving in hdfs.\

\b \ul Solution:
\b0 \ulnone \
Step 1 : Create al1three files in hdfs (We wil1do using Hue). However, you can first create in loca1filesystem and then upload it to hdfs. \

\b \ul Step 2 : Load content from al1files. 
\b0 \ulnone \
va1content = sc.textFile("spark3/sparkdir1/file1.txt,spark3/sparkdir2/file2.txt,spark3/sparkdir3/file3.txt") //Load the text file \

\b \ul Step 3 : Now create split each line and create RDD of words. 
\b0 \ulnone \
va1flatContent = content.flatMap(word=>word.split(" ")) \

\b \ul Step 4 : Remove space after each word (trim it) 
\b0 \ulnone \
va1trimmedContent = flatcontent.map(word=>word.trim) \

\b \ul Step 5 : Create an RDD from remove, al1the words that needs to be removed. 
\b0 \ulnone \
va1removeRDD =sc.parallelize(List("a","the","an", "as", "a","with","this","these","is","are","in","for","to","and","the","of"))\

\b \ul Step 6 : Filter the ROD, so it can have only content which are not present in removeRDD. 
\b0 \ulnone \
va1filtered = trimmedContent.subtract(removeRDD) \

\b \ul Step 7 : Create a PairRDD, so we can have (word,l) tuple or PairRDD
\b0 \ulnone . \
va1pairRDD = filtered.map(word => (word,l)) \

\b \ul Step 8 : Now do the word count on PairRDD. 
\b0 \ulnone \
va1wordCount = pairRDD.reduceByKey(_ + _) \

\b \ul step 9 : NOW swap PairRDD. 
\b0 \ulnone \
va1swapped = wordCount.map(item => item. swap) \

\b \ul Step 10 : Now revers order the content
\b0 \ulnone . \
va1sortedOutput = swapped.sortByKey(false) \

\b \ul Step 11 : Save the output as a Text file. 
\b0 \ulnone \
sortedOutput.saveAsTextFile("spark3/result") \

\b \ul Step 12 : Save compressed output.
\b0 \ulnone  \
import org.apache.hadoop.io.compress.GzipCodec\
sortedOutput.saveAsTextFile("spark3/compressedresult", classOt[GzipCodec]) \
================================================================================================================================\
\

\b \ul Problem Scenario 40: You have given a files as below.
\b0 \ulnone \
\
Spark5/EmployeeName.csv (id,name)\
E01,Lokesh\
E02,Bhupesh\
E03,Amit\
E04,Ratan\
E05,Dinesh\
E06,Pavan\
E07,Tejas\
E08,Sheela\
E09,Kumar\
E10,Venkat\
\
Spark5/EmployeeSalary.csv (Id, Salary)\
E01,50000\
E02,50000\
E03,45000\
E04,45000\
E05,50000\
E06,45000\
E07,50000\
E08,10000\
E09,10000\
E10,10000\
\
Now write a spark code in scala which wil1load these two files from hdfs and join the same, and produce the (name,salary) values.\
And save the data in multiple file group by salary ( Means each file will have name of employees with same salary). Make sure file name include salary as well.\

\b \ul \
Solution:
\b0 \ulnone \
\

\b \ul Step 1 : Create al1three files in hdfs (We wil1do using Hue). However, you can first create in loca1filesystem and then upload it to hdfs.
\b0 \ulnone  \

\b \ul Step 2 : Load EmployeeName.csv file from hdfs and create PairRDDs 
\b0 \ulnone \
va1name = sc.textFile("spark5/EmployeeName.csv") \
va1namePairRDD = name.map(x=> (x.split(",")(O),x.split(",")(1))))\
Step 3 : Load Employe\
\
================================================================================================================================\
\

\b \ul Problem Scenario 37: 
\b0 \ulnone you have been given three csv files in hdfs as below.\
\
EmployeeName.csv with the field ( id, name)\
EmployeeManager.csv (id, managerName)\
EmployeeSalary.cscv (id, Salary)\
EmployeeManager.csv\
E01,Vishnu\
E02,Satyam\
E03,Shiv\
E04,Sundar\
E05,John\
E06,Pallavi\
E07,Tanvir\
E08,Shekhar\
E09,Vinod\
E10,Jitendra\
EmployeeName.csv\
E01,Lokesh\
E02,Bhupesh\
E03,Amit\
E04,Ratan\
E05,Dinesh\
E06,Pavan\
E07,Tejas\
E08,Sheela\
E09,Kumar\
E10,Venkat\
EmployeeSalary.csv\
E01,50000\
E02,50000\
E03,45000\
E04,45000\
E05,50000\
E06,45000\
E07,50000\
E08,10000\
E09,10000\
E10,10000\
Using Spark and its API you have to generate a joined output as below and save as a text file (Separated by comma) for fina1distribution and output must be sorty by id.\
Id,name,salary,managerName\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create al1three files in hdfs in directory called spark1(We wil1do using Hue). However, you can first create in loca1filesystem and then upload it to hdfs.\

\b \ul Step 2 
\b0 \ulnone : Load EmployeeManager.csv file from hdfs and create PairRDDs \
valmanager = sc.textFile("spark1/EmployeeManager.csv") \
valmanagerPairRDD = manager.map(x=> (x.split(",")(0),x.split(",")(1)))) \

\b \ul Step 3 :
\b0 \ulnone  Load EmployeeName.csv file from hdfs and create PairRDDs \
valname = sc.textFile("spark1/EmployeeName.csv") \
valnamePairRDD = name.map(x=> (x.split(",")(O),x.split(",")(1))) \

\b \ul Step 4 :
\b0 \ulnone  Load Employeesalary.csv file from hdfs and create PairRDDs \
valsalary = sc.textFile("spark1/EmployeeSalary.csv") \
valsalaryPairRDD = salary.map(x=> (x.split(",")(O),x.split(",")(1)))) \

\b \ul step 4 : 
\b0 \ulnone Join al1pairRDDS \
valjoined = namePairRDD.join(salaryPairRDD).join(managerPairRDD) \

\b \ul Step 5 :
\b0 \ulnone  Now sort the joined results. \
valjoinedData = joined.sortByKey() \

\b \ul Step 6 : 
\b0 \ulnone Now generate comma separated data. \
valfinalData = joinedData.map(v=> (v._1,v._2._1._1,v._2._1._2,v._2._2)) \

\b \ul Step 7 :
\b0 \ulnone  Save this output in hdfs as text file. \
finalData.saveAsTextFile("spark1/result.txt") \
=================================================================================================================================\

\b \ul Problem Scenario 38: 
\b0 \ulnone You have given following two files,\
1. Content.txt : Contain a huge text file containing space separated words.\
Hello this is HadoopThoughtworks.com \
This is HadoopThoughtworks.com\
Apache Spark Training\
This is Spark Learning Session\
2.Remove.txt : Ignore/filter al1the words given in this file (Comma Separated).\
Hello, is, this, the\
Write a Spark program which reads the content.txt and load as an RDD, Remove al1the words from a broadcast variables (Which is loaded as an RDD of words from Remove.txt). And count the occurrentce of the each word  and save it as a text file in HDFS.\
Spark is faster than MapReduce\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create all three files in hdfs in directory called spark2 (We wil do using Hue). However, you can first create in local filesystem and then upload it to hdfs.\

\b \ul step 2 : 
\b0 \ulnone Load the Content.txt file \
valcontent = sc.textFile("spark2/Content.txt") //Load the text file \

\b \ul Step 3 :
\b0 \ulnone  Load the Remove.txt file \
valremove = sc.textFile("spark2/Remove.txt") //Load the text tile \

\b \ul Step 4 : 
\b0 \ulnone Create an RDD from remove, However, there is a possibility each word could have trailing spaces, remove those whitespaces as well.We have used two functions here flatMap,map and trim\
valremoveRDD= remove.flatMap(x=> x.split(",") ).map(word=>word.trim)//Create an array of words \

\b \ul Step 5 : 
\b0 \ulnone Broadcast the variable, which you want to ignore \
valbRemove = sc.broadcast(removeRDD.collect().toList) // It should be array ot Strings \

\b \ul Step 6 :
\b0 \ulnone  Split the content ROD, so we can have Array ot String. \
valwords = content.flatMap(line => line.split(" ")) \

\b \ul Step 7 :
\b0 \ulnone  Filter the RDD, so it can have only content which are not present in "Broadcast Variable". \
valfiltered = words.filter\{case (word) => !bRemove.value.contains(word)\} \

\b \ul Step 8 :
\b0 \ulnone  Create a PairRDD, so we can have (word,l) tuple or PairRDD. \
valpairRDD = filtered.map(word => (word,l)) \

\b \ul Step 9 :
\b0 \ulnone  Now do the word count on PairRDD. \
valwordCount = pairRDD.reduceByKey(_ + _) \

\b \ul Step 10 :
\b0 \ulnone  Save the output as a Text file. \
wordCount.saveAsTextFile("spark2/result.txt") \
=====================================================================================================================================\

\b \ul Problem Scenario 39 :
\b0 \ulnone  You have given three file as below.\
spark3/sparkdir1/file1.txt\
Apache Hadoop is an open-source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. Al1the modules in Hadoop are designed with a fundamenta1assumption that hardware failures are common and should be automatically handled by the framework\
spark3/sparkdir2/file2.txt\
The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File System (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers packaged code for nodes to process in paralle1based on the data that needs to be processed.\
spark3/sparkdir3/file3.txt\
This approach takes advantage of data locality nodes manipulating the data they have access to to allow the dataset to be processed faster and more efficiently than it would be in a more conventiona1supercomputer architecture that relies on a paralle1file system where computation and data are distributed via high-speed networking\
Each file contain some text.\
Now write a Spark code in scala which wil1load al1these three files from hdfs and do the word count by filtering following words.\
And result should be sorted by word count in reverse order.\
Filter words ("a","the","an","as","a","with","this","these","is","are","in","for","to","and","The","of")\
Also please make sure you load al1three files as a Single RDD ( Al1three files must be loaded using single API call).\
You have also been given following codec\
\ul import org.apache.hadoop.io.compress.GzipCodec\ulnone \
Please use above codec to compress file, while saving in hdfs.\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create al1three files in hdfs (We wil1do using Hue). However, you can first create in loca1filesystem and then upload it to hdfs. \

\b \ul Step 2 : 
\b0 \ulnone Load content from all files. \
valcontent = sc.textFile("spark3/sparkdir1/file1.txt,spark3/sparkdir2/file2.txt,spark3/sparkdir3/file3.txt") //Load the text file \

\b \ul Step 3 : 
\b0 \ulnone Now create split each line and create RDD of words. \
valflatContent = content.flatMap(word=>word.split(" ")) \

\b \ul Step 4 :
\b0 \ulnone  Remove space after each word (trim it) \
valtrimmedContent = flatcontent.map(word=>word.trim) \

\b \ul Step 5 :
\b0 \ulnone  Create an RDD from remove, al1the words that needs to be removed. \
valremoveRDD =sc.parallelize(List("a","the","an", "as", "a","with","this","these","is","are","in","for","to","and","the","of"))\

\b \ul Step 6 :
\b0 \ulnone  Filter the ROD, so it can have only content which are not present in removeRDD. \
valfiltered = trimmedContent.subtract(removeRDD) \

\b \ul Step 7 :
\b0 \ulnone  Create a PairRDD, so we can have (word,l) tuple or PairRDD. \
valpairRDD = filtered.map(word => (word,l)) \

\b \ul Step 8 :
\b0 \ulnone  Now do the word count on PairRDD. \
valwordCount = pairRDD.reduceByKey(_ + _) \

\b \ul step 9 : 
\b0 \ulnone NOW swap PairRDD. \
valswapped = wordCount.map(item => item. swap) \

\b \ul Step 10 :
\b0 \ulnone  Now revers order the content. \
valsortedOutput = swapped.sortByKey(false) \

\b \ul Step 11 : 
\b0 \ulnone Save the output as a Text file. \
sortedOutput.saveAsTextFile("spark3/result") \

\b \ul Step 12 :
\b0 \ulnone  Save compressed output. \
import org.apache.hadoop.io.compress.GzipCodec \
sortedOutput.saveAsTextFile("spark3/compressedresult", classOf[GzipCodec]) \
\
================================================================================================================================\

\b \ul Problem Scenario 40:
\b0 \ulnone  You have given a files as below.\
Spark5/EmployeeName.csv (id,name)\
E01,Lokesh\
E02,Bhupesh\
E03,Amit\
E04,Ratan\
E05,Dinesh\
E06,Pavan\
E07,Tejas\
E08,Sheela\
E09,Kumar\
E10,Venkat\
Spark5/EmployeeSalary.csv (Id, Salary)\
E01,50000\
E02,50000\
E03,45000\
E04,45000\
E05,50000\
E06,45000\
E07,50000\
E08,10000\
E09,10000\
E10,10000\
Now write a spark code in scala which will load these two files from hdfs and join the same, and produce the (name,salary) values.\
And save the data in multiple file group by salary ( Means each file will have name of employees with same salary). Make sure file name include salary as well.\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create al1three files in hdfs (We wil1do using Hue). However, you can first create in local filesystem and then upload it to hdfs. \

\b \ul Step 2 :
\b0 \ulnone  Load EmployeeName.csv file from hdfs and create PairRDDs \
valname = sc.textFile("spark5/EmployeeName.csv") \
valnamePairRDD = name.map(x=> (x.split(",")(O),x.split(",")(1))))\

\b \ul Step 3 
\b0 \ulnone : Load Employeesalary.csv file trom hdfs and create PairRDDs \
valsalary = sc.textFile("spark5/EmployeeSalary.csv") \
valsalaryPairRDD = salary.map(x=> (x.split(",")(O),x.split(",")(1))) \

\b \ul step 4 :
\b0 \ulnone  J0in al1pairRDDS \
valjoined = namePairRDD.join(salaryPairRDD) \

\b \ul Step 5 :
\b0 \ulnone  Remove key from ROD and Salary as a Key. \
valkeyRemoved = joined.values \

\b \ul Step 6 
\b0 \ulnone : Now swap filtered RDD. \
valswapped = keyRemoved.map(item => item.swap) \

\b \ul Step 7 :
\b0 \ulnone  Now groupBy keys (It wil1generate key and value array) \
va1grpByKey = swapped.groupByKey().collect() \

\b \ul Step 8 
\b0 \ulnone : Now create RDD for values collection \
valrddByKey = grpByKey.map\{case (k,v) => k->sc.makeRDD(v.toSeq)\} \

\b \ul Step 9 : 
\b0 \ulnone Save the output as a Text file. \
rddByKey.foreach\{ case (k,rdd) => rdd.saveAsTextFile("spark5/Employee"+k)\} \
====================================================================================================================================\

\b \ul Problem Scenario 41 :
\b0 \ulnone  You have given a file named spark6/user.csv.\
id,topic,hits\
Rahul,scala,120\
Nikita,spark,80\
Mithun,spark,1\
myself,cca175,180\
Now write a Spark code in scala which wil1remove the header part  and create RDD of values as below, for al1rows, And also if id is "myself" than filter out now.\
Map(id -> om, topic -> scala, hits -> 120)\
\

\b \ul Step 1 : 
\b0 \ulnone Create file in hdfs (We wil1do using Hue). However, you can first create in local filesystem and then upload it to hdfs. \

\b \ul Step 2 :
\b0 \ulnone  Load user.csv file from hdfs and create PairRDDs \
valcsv = sc.textFile("sparkS/user.csv") \

\b \ul Step 3 :
\b0 \ulnone  split and clean data \
valheaderAndRows = csv.map(line => line.split(",").map(_.trim)) \

\b \ul Step 4 :
\b0 \ulnone  Get header row \
va1header = headerAndRows.first \

\b \ul Step 5 :
\b0 \ulnone  Filter out header (We need to check if the first va1matches the first header name) \
valdata = headerAndRows.filter(_(O)!= header(O)) \

\b \ul Step 6 : 
\b0 \ulnone Splits to map (header/value pairs) \
valmaps = data.map(splits => header.zip(splits).toMap) \

\b \ul Step 7 :
\b0 \ulnone  Filter out the user "myself" \
valresult = maps.filter(map => map("id")!= "myself") \

\b \ul Step 8 : 
\b0 \ulnone Save the output as a Text file. \
result.saveAsTextFile("sparkS/result.txt") \
---------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 42:
\b0 \ulnone  You have been given a file named spark7/EmployeeName.csv (id,name).\
E01,Lokesh\
E02,Bhupesh\
E03,Amit\
E04,Ratan\
E05,Dinesh\
E06,Pavan\
E07,Tejas\
E08,Sheela\
E09,Kumar\
E10,Venkat\
1.Load this file from hdfs and sort it by name and save it back as (id,name) in results directory. However, Makes sure while saving it should be able to write in a single file.\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1:
\b0 \ulnone Create file in hdfs(We will do using Hue).However,you can first create in local filesystem and then upload it to hdfs.\

\b \ul Step 2:
\b0 \ulnone Load EmployeeName.csv file from hdfs and create PairRDDs\
valname = sc.textFile("spark7/EmployeeName.csv")\
valnamePairRDD = name.map(x=>(x.split(",")(0),x.spilt(",")(1)))\

\b Step 3:
\b0 Now swap namePairRDD RDD.\
valswapped = namePairRDD.map(item=>item.swap)\

\b \ul Step 4:
\b0 \ulnone Now sort the rdd by key.\
valsortedOutput = swapped.sortByKey()\

\b \ul Step 5:
\b0 \ulnone Now swap the result back\
valswappedBack = sortedOutput.map(item=>item.swap)\

\b \ul Step 6:
\b0 \ulnone Save the output as a Text file and output must be written in a single file.\
swappedBack.repartition(1).saveAsTextFile("spark7/result.txt")\
\
-----------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 43:
\b0 \ulnone  You have been given a file named spark8/data.csv (type,name).\
1,Lokesh\
2,Bhupesh\
2,Amit\
2,Ratan\
2,Dinesh\
1,Pavan\
1,Tejas\
2,Sheela\
1,Kumar\
1,Venkat\
1. Load this file from hdfs and save it back as (id, (al1names of same type)) in results directory. However, make sure while saving it should be able to write in a single file.\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create file in hdfs (We wil1do using Hue). However, you can first create in loca1filesystem and then upload it to hdfs. \

\b \ul Step 2 :
\b0 \ulnone  Load data.csv file from hdfs and create PairRDDs \
va1name = sc.textFile("spark8/data.csv") \
va1namePairRDD = name.map(x=> (x.split(",")(O),x.split(",")(1))) \

\b \ul step 3 : 
\b0 \ulnone NOW swap namepairRDD ROD. \
va1swapped = namePairRDD.map(item => item.swap) \
\
Step 4 : Now combine the rdd by key. \
va1combinedOutput = namePairRDD.combineByKey(List(_), (x:List[String], y:String)=> y :: x, (x:List[String], y:List[String]) => x ::y) \
\
Step 5 : Save the output as a Text file and output must be written in a single file. \
\
combinedOutput.repartition(1).saveAsTextFile(" spark8/result.txt") \
\
\
-----------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 44 : HadoopThoughtworks.com has done survey on their Thoughtworks products feedback using a web based form, with the following free text field as input in web ui.\
\
Name: String\
\
Subscription Date : String\
\
Rating : String\
\
\
\
And servey data has been saved in a file called Spark9/feedback.txt\
\
Christopher|Jan 11, 2015|5 \
\
Kapil|11 Jan, 2015|5 \
\
Thomas|6/17/2014|5 \
\
John|22-08-2013|5 \
\
Mithun|2013|5 \
\
Jitendra||5\
\
\
\
Write a spark program using regular expression which wil1filter al1the vaild dates and save in tw separate file (good record and bad record)\
\
---------------------------------------------------------------------------------------------------------------------\
Solution : \
\
Step 1 : Create a file first using Hue in hdfs.\
Step 2 : Write al1valid regular expressions sysntex for checking whether records are having valid dates or not. \
\
va1reg1="""(\\d+)\\s(\\w(3))(,)\\s(\\d\{4\})""".r//11 jan,2015\
va1reg2="""(\\d+)(\\/)(\\d+)(\\/)(\\d\{4\})""".r//6/17/2014\
va1reg3="""(\\d+)(-)(\\d+)(-)(\\d\{4\})""".r//22-08-2013\
va1reg4="""(\\w\{3\})\\s(\\d+)(,)\\s(\\d\{4\})""".r//Jan 11,2015\
\
Step 3 : Load the file as an ROD. \
va1feedbackRDD = sc.textFile("spark9/feedback.txt") \
\
Step 4 : As data are pipe separated , hence split the same. \
va1feedbackSplit = feedbackRDD.map(line => line.split('|')) \
\
Step 5 : Now get the valid records as wel1as , bad records. \
va1validRecords = feedbackSplit.filter(x => (reg1.pattern.matcher(x(l).trim).matches|reg2.pattern.matcher(x(1).trim).matches|reg3.pattern.matcher(x(1).trim).matches|reg4.pattern.matcher(x(1).trim).matches)) \
va1badRecords = feedbackSplit.filter(x => !(regl.pattern.matcher(x(l).trim).matches|reg2.pattern.matcher(x(1).trim).matches|reg3.pattern.matcher(x(1).trim).matches|reg4.pattern.matcher(x(1).trim).matches)) \
\
Step 6 : Now convert each Array to Strings \
va1valid =validRecords.map(e => (e(O),e(1),e(2))) \
va1bad =badRecords.map(e => (e(O),e(1),e(2))) \
\
Step 7 : Save the output as a Text file and output must be written in a single file. \
valid.repartition(1).saveAsTextFile("spark9/good.txt") \
bad.repartition(1).saveAsTextFile("spark9/bad.txt") \
\
------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 45 : You have been given an RDD as below.\
\
va1rdd: RDD[Array[Byte]]\
\
\
\
Now you have to save this RDD as a Sequence file. And below is the code snippet.\
\
\
\
Import  orgg.apache.hadoop.io.compress.GzipCodec\
\
rdd.map(byteArray => (A.get(), new B(bytesArray))),saveAsSequenceFile("/output/path",classOf[GzipCodec])\
\
\
\
What would be the correct replacement for A and B in above snippet.\
\
---------------------------------------------------------------------------------------------------------------------------\
Solution:\
A.NullWritable\
B.BytesWritable\
-------------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 46 : You have been given two files\
\
\
\
Spark16/file1.txt\
\
1,9,5\
\
2,7,4\
\
3,8,3\
\
\
\
spark16/file2.txt\
\
1,g,h\
\
2,i,j\
\
3,k,l\
\
\
\
load these two files as Spark RDD and Join them to produce the below results.\
\
(1, ( (9,5), (g,h) ))\
\
(2, ( (7,4), (i,j) ))\
\
(3, ( (8,3), (k,l) ))\
\
\
\
And write code snippet which wil1sum the second columns of above joined results (5+4+3).\
\
--------------------------------------------------------------------------------------------------------------------------\
Solution : \
\
Step 1 : Create files in hdfs using Hue. \
\
Step 2 : Create pairRDD tor both the files. \
va1one = sc.textFile("spark16/file1.txt").map\{ \
_.split(",", -1) match \{ \
case Array(a, b, c) => (a, ( b, c)) \
\}\
\}\
\
va1two = sc.textFile("spark16/file2.txt").map\{ \
_.split(",", -1) match \{ \
case Array(a, b, c) => (a, (b, c)) \
\}\
\}\
\
step 3 : Join both the ROD. \
va1joined = one.join(two) \
\
Step 4 : Sum second column values. \
va1sum = joined.map \{ \
case (_, ((_, num2), (_, _))) => num2.toint \
\}.reduce(_+_)\
\
\
-----------------------------------------------------------------------------------------------------------------------------\
\
Probelm Scenario 47. You have been given sample data as below in a file called spark15/file1.txt\
\
\
\
3070811,1963,1096,,"US","CA",,1,\
\
3022811,1963,1096,,"US","CA",,1,56\
\
3033811,1963,1096,,"US","CA",,1,23\
\
\
\
Below is the code snippet to process this file.\
\
\
\
va1field= sc.textFile("spark15/file1.txt")\
\
va1mapper = field.map(x=> A)\
\
mapper.map(x => x.map(x=> \{B\})).collect\
\
\
\
Please fil1in A and B so it can generate below finaloutput\
\
Array(Array(3070811, 1963, 1096, 0, "US", "CA", 0, 1, 0)\
\
,Array(30222811, 1963, 1096, 0, "US", "CA", 0, 1, 56)\
\
,Array(3033811, 1963, 1096, 0, "US", "CA", 0, 1, 23)\
\
)\
\
-----------------------------------------------------------------------------------------------------------------------------\
Solution:\
A.x.split(",",-1)\
B.if(x.isEmpty)0 else x\
---------------------------------------------------------------------------------------------------------------------------------\
\
\
\
problem Scenario 48: You have been given below code snippet.\
\
\
\
Va1au1 = sc.parallelize(List ( ("a" , Array(1,2)) , ("b" , Array(1,2))))\
\
Va1au1 = sc.parallelize(List ( ("a" , Array(3)) , ("b" , Array(2))))\
\
\
\
Apply the spark method which wil1generate below output\
\
Array[(String, Array[int])] = Array((a,Array(1,2)), (b,Array(1,2)), (a,Array(3)), (b, Array(2)))\
\
--------------------------------------------------------------------------------------------------------------------------------\
Solution:\
au1.union(au2)\
------------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 49 : you have been given a file (spark10/sales.txt), \
\
Department,Designation,costToCompany,State\
\
Sales,Trainee,12000,UP\
\
Sales,Lead,32000,AP\
\
Sales,Lead,32000,LA\
\
Sales,Lead,32000,TN\
\
Sales,Lead,32000,AP\
\
Sales,Lead,32000,TN \
\
Sales,Lead,32000,LA\
\
Sales,Lead,32000,LA\
\
Marketing,Associate,18000,TN\
\
Marketing,Associate,18000,TN\
\
HR,Manager,58000,TN\
\
\
\
And want to produce the output as a csv with group by Department, Dessignation,State with additiona1columns with sum(costToCompany) and TotalEmployeeCount\
\
\
\
Should get result like\
\
\
\
Dept,Desg,state,empCount,totalCost\
\
sales,Lead,AP,2,64000\
\
Sales,Lead,LA,3,96000\
\
Sales,Lead,TN,2,64000\
\
----------------------------------------------------------------------------------------------------\
Solution : \
\
Step 1 : Create a file first using Hue in hdfs. \
\
step 2 : Load file as an RDD \
va1rawlines = sc.textFile("spark10/sales.txt") \
\
Step 3 : Create a case class, which can represent its column fileds. \
case class Employee(dep: String, des: String, cost: Double, state: String) \
\
Step 4 : Split the data and create RDD of al1Employee objects. \
va1employees = rawlines.map(_.split(",")).map(row =>Employee(row(O), row(l), row(2).toDouble, row(3))) \
\
Step 5 : Create a row as we needed. Al1group by fields as a key and value as a count tor each employee as wel1as its cost. \
va1keyVals = employees.map( em => ((em.dep, em.des, em.state), (1 , em.cost))) \
\
Step 6 : Group by al1the records using reduceByKey method as we want summation as well. For number of employees and their tota1cost. \
va1results = keyVals.reduceByKey\{ (a,b) => (a._1+ b. _ 1, a._2 + b. _2)\} I1(a.count + b.count , a.cost + b.cost )\} \
\
Step 7 : Save the results in a text tile as below. \
results.repartition(1).saveAsTextFile("spark10/group.txt") \
\
---------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 50 : you have been given following code snippet.\
\
\
\
Va1grouped = sc.parallelize(Seq(((1,"two"), List((3,4), (5,6)))))\
\
va1flattened = grouped.flatMap \{ A =>\
\
groupvalues.map \{ value => B\}\
\
\}\
\
\
\
You need to generate following output. Hence replace A and B\
\
Array((1,two,3,4), (1,two,5,6))\
\
---------------------------------------------------------------------------------------------------------------------\
Solution:\
A.case(key,groupValues)\
B.(key._1,key._2,value._1,value._2)\
-----------------------------------------------------------------------------------------------------------------------\
\
problem Scenario 51 : you have been given 4 files, with the content as given in RHS\
\
spark11/file1.txt\
\
Apache Hadoop is an open-source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. Al1the modules in Hadoop are designed with a fundamenta1assumption that hardware failures are common and should be automatically handled by the framework\
\
\
\
spark11/file2.txt\
\
The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File System (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers packaged code for nodes to process in paralle1based on the data that needs to be processed.\
\
\
\
spark11/file3.txt\
\
his approach takes advantage of data locality nodes manipulating the data they have access to to allow the dataset to be processed faster and more efficiently than it would be in a more conventiona1supercomputer architecture that relies on a paralle1file system where computation and data are distributed via high-speed networking\
\
\
\
spark11/file4.txt\
\
Apache Storm is focused on stream processing or what some cal1complex event processing. Storm implements a fault tolerant method for performing a computation or pipelining multiple computations on an event as it flows into a system. One might use Storm to transform unstructured data as it flows into a system into a desired format.\
\
\
\
Write a Spark program, which wil1give you the highest occuring words in each file. which their file name and highest occuring words.\
\
---------------------------------------------------------------------------------------------------------------------------\
Solution : \
\
Step 1 : Create al14 file first using Hue in hdfs. \
\
step 2 : Load al1file as an ROD \
va1file1= sc.textFile("spark11/file1.txt") \
va1file2 = sc.textFile("spark11/file1.txt") \
va1file3 = sc.textFile("spark11/file1.txt") \
va1file4 = sc.textFile("spark11/file1.txt") \
\
Step 3 : Now do the word count for each file and sort in rverse order of count. \
va1content1= file1.flatMap( line => line.split(" ")).map(word => (word,1)).reduceByKey(_ + _).map(item => item.swap).sortByKey(talse).map(e=> \
va1content2 = file2.flatMap( line => line.split(" ")).map(word => (word,1)).reduceByKey(_ + _).map(item => item.swap).sortByKey(talse).map(e=> \
va1content3 = file3.flatMap( line => line.split(" ")).map(word => (word,1)).reduceByKey(_ + _).map(item => item.swap).sortByKey(talse).map(e=> \
va1content4 = file4.flatMap( line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _).map(item => item.swap).sortByKey(talse).map(e=> \
\
Step 4 : Split the data and create RDD of al1Employee objects. \
\
va1file1word =sc.makeRDD(Array(file2.name+"->"+content1(O)._1+"-"+content1(0)._2)) \
\
va1file2word = sc.makeRDD(Array(file2.name+"->"+content2(O)._1+"-"+content2(0)._2)) \
\
va1file3word = sc._makeRDD(Array(tile3.name+"->"+content3(O)._1+"-"+content2(0)._2)) \
\
va1file4word = sc.makeRDD(Array(tile4.name+"->"+content4(O)._1+"-"+content2(0)._2)) \
\
step 5 : union al1the RDDS \
va1unionRDDs = file1word.union(file2word).union(file3word).union(file4word) \
\
Step 6 : Save the results in a text file as below. \
unionRDDs.repartition(1).saveAsTextFile("spark11/union.txt)\
\
---------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 52 : You have been given 2 files, with the content as given in RHS\
\
(spark12/technology.txt)\
\
first,last,technology\
\
Amit,Jain,java\
\
Lokesh,kumar,unix\
\
Mithun,kale,spark\
\
Rajni,vekat,hadoop\
\
Rahul,Yadav,scala\
\
\
\
(spark12/salary.txt)\
\
first,last,salary\
\
Amit,Jain,100000\
\
Lokesh,kumar,95000\
\
Mithun,kale,150000\
\
Rajni,vekat,154000\
\
Rahul,Yadav,120000\
\
\
\
Write a Spark program, which wil1join the data based on first and last name and save the joined results in following format.\
\
first,last,technology,salary\
\
---------------------------------------------------------------------------------------------------------------------------------\
Solution : \
\
Step 1 : Create 2 files first using Hue in hdfs. \
\
step 2 : Load al1file as an RDD \
va1technology = sc.textFile("spark12/technology.txt").map(e => e.split(",")) \
va1salary = sc.textFile("spark12/salary.txt").map(e => e.split(",")) \
\
Step 3 : Now create Key,value pair of data and join them. \
va1joined = technology.map(e=>((e(0).e(1)),e(2))).jion(salary.map(e=>((e(0),e(1)),e(2)))\
\
Step 4 : Save the results in a text tile as below. \
joined. repartition (1). saveAsTextFile(" spark121multiColumnJoined.txt") \
---------------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 53 : You have been given below list in scala (name,sex,cost) for each work done\
\
List( ("Deeapak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female", 2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000) , ("Neeta" , "female", 2000))\
\
\
\
Now write a Spark program to load this list as an RDD and do the sum of cost for combination of name and sex (as key)\
\
---------------------------------------------------------------------------------------------------------------------------------\
Solution : \
\
Step 1 : Create an RDD out of this list \
\
va1rdd = sc.parallelize(List( ("Deeapak" , "male' ,4000), ("Deepak" , "female",2000),("Deepak","female",2000),("Deepak","male",1000),("Neeta","female",2000)))\
\
step 2 : convert this RDD in pair RDD \
va1byKey = rdd.map(\{case (name,sex,cost) => (name,sex)->cost\}) \
\
Step 3 : Now group by Key \
va1byKeyGrouped = byKey.groupByKey \
\
Step 4 : Now sum the cost for each group \
va1result = byKeyGrouped.map\{case ((id1,id2),values) => (id1,id2,values,sum)\} \
\
Step 5 : Save the results \
result. repartition(1).saveAsTextFile("spark12/result.txt") \
---------------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 54: You have been given below code snippet, with imtermediate output.\
\
\
\
Va1z = sc.parallelize(List(1,2,3,4,5,6), 2)\
\
\
\
//lets first print out the contents of the RDD with partition labels\
\
def myfunc(index: Int, iter: Iterator[(Int)]) : Iterator[String] = \{\
\
iter.toList.map(x => "[partID:"+index+", val: "+x+"]).iterator\
\
\}\
\
\
\
//In each run, output could be different, while solving problem assume below output only.\
\
z.mapPartitionsWithInde(myfunc).collect\
\
res28: Array[String]= Array([partID:0, Val: 1], [partID:0, val:2], [partID:0, val:3], [partID:1, Val: 4], [partID:1, val:5], [partID:1, val:6])\
\
\
\
Now apply aggreate method on RDD z, with two reduce function, first wil1select max value in each partition and second wil1add al1the maximum values from al1parttions.\
\
Initialize the aggregate with value 5, hence expected output wil1be 16.\
\
--------------------------------------------------------------------------------------------------------------------------------\
Solution:\
z.aggregate(5)(math.max(_,_),_+_)\
---------------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 55 : You have been given below python code snippet, with intermediate output.\
\
We want to take a list of records about people and then we want to sum up their ages and count them.\
\
So for this Thoughtworksple the type in the RDD wil1be a Dictionary in the format of \{name: NAME, age: AGE, gender:GENDER\}.\
\
The result type wil1be a tuple that looks like so (sum of Ages, Count)\
\
\
\
people = []\
\
people.append(\{'name':'Amit', 'age':45,'gender':'M'\})\
\
people.append(\{'name':'Ganga', 'age':43,'gender':'F'\})\
\
people.append(\{'name':'John', 'age':28,'gender':'M'\})\
\
people.append(\{'name':'Lolita', 'age':33,'gender':'F'\})\
\
people.append(\{'name':'Dont Know', 'age':18,'gender':'T'\})\
\
\
\
peopleRDD=sc.parallelixe(people) //Create an RDD\
\
\
\
PeopleRDD.aggregate((0,0), seqOp, combOp)\
\
//Output of above line : 167, 5)\
\
\
\
Now define two operation seqOp and comb)p, such that\
\
\
\
seqOp: Sum the age of al1people as wel1count them, in each partition.\
\
combOp : Combine results from al1partitions.\
\
-------------------------------------------------------------------------------------------------------------------------------\
Solution:\
seqOp = (lambda x,y:(x[0]+y['age'],x[1]+1))\
combOp = (lambda x,y:(X[0]+y[0],x[1]+y[1]))\
-------------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 56 : You have been given below code snippet (do a sum of values by key), with intermediate output.\
\
\
\
va1keysWithValuesList = Array("foo=A", "foo=A", "foo=A", "foo=A", "foo=B", "bar=C", "bar=D", "bar=D")\
\
Va1data = sc.parallelize(keysWithValuesList)\
\
//Create key value pairs\
\
va1kv = data.map(_.split("=")).map(v => (v(0), v(1))).cache()\
\
\
\
Va1initialCount = 0;\
\
Va1countByKey = kv.aggregateByKey(initialCount)(addToCounts, sumPartitionCounts)\
\
\
\
Now define two functions (addToCounts, sumPartitionCounts) such, which wil1produce following results.\
\
\
\
Output 1\
\
countByKey.collect\
\
res3: Array[(String, Int)] = Array((foo,5), (bar,3))\
\
\
\
import scala.collection._\
\
va1intialSet = scala.collection.mutable.HashSet.empty[String]\
\
va1uniqueBykey = kv.aggregateByKey(initialSet)(addToSet, mergePartitionSets)\
\
\
\
Now define two functions (addToSet, mergePartitionSets) such, which wil1produce following results.\
\
\
\
Output 2:\
\
uniqueByKey.collect\
\
res4: Array[(String, scala.collection.mutable.HashSet[String])] = Aray((foo,Set(B, A)), (bar,Set(c, D)))\
\
------------------------------------------------------------------------------------------------------------------------\
Solution:\
va1addToCounts = (n: Int, v: String)=> n + 1 \
va1sumPartitionCounts = (P1: Int, p2: Int) => p1+ p2 \
\
va1addToSet = (s: mutable.HashSet[String], v: String) => s+= v \
va1mergePartitionSets = (P1: mutable.HashSet[String], p2: mutable.HashSet[String]) => pl++= p2 \
--------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 57 : You have been given below code snippet (calculating an average score), with intermediate output.\
\
\
\
type ScoreCollector = (Int, Double)\
\
type PersonScores = (String, (Int, Double))\
\
\
\
va1initialScores = Array(("Fred", 88.0), ("Fred", 95.0), ("Fred", 91.0), ("Wilma", 93.0), ("Wilma", 95.0), ("Wilma", 98.0))\
\
va1wilmaAndFredScores = sc.parallelize(initialScores).cache()\
\
\
\
va1scores = wilmaAndFredScores.combineByKey(createScoreCombiner, ScoreCombiner, scoreMerger)\
\
\
\
va1averagineFunction = (personScore: PersonScores) => \{\
\
va1(name, (numberScores, totalScore)) = personScore\
\
(name, totalScore / numberScores)\
\
\}\
\
\
\
va1averageScores = scores.collectAsMap().map(averagingFunction)\
\
Expected output : acerageScores: scala.collection.Map[String,Double] = Map(Fred -> 91.33333333333333, Wilma -> 95.33333333333333)\
\
\
\
Define al1three required function, which are input for combineByKey method. e.g (create ScoreCombiner, scoreCombiner, scoreMerger). And help us producing required results.\
\
-----------------------------------------------------------------------------------------------------------------------------\
Solution : \
\
va1createScoreCombiner = (score: Double) => (1, score) \
va1scorecombiner= (collector: Scorecollector, score: Double) => \{ \
va1(numberscores, totalScore) = collector \
(numberscores + 1, totalScore + score) \
\}\
va1scoreMerger = (collectorl: Scorecollector, collector2: Scorecollector) => \{ \
va1(numScores1, totalScore1) = collector1\
va1(numScores2, totalScore2) = collector2 \
(numScores1 + numScores2, totalScore1 + totalScore2) \
\}\
\
Description : \
The createScoreCombiner takes a double value and returns a tuple of (Int, Double) \
\
The scorecombiner tunction takes a Scorecollector which is a type alias tor a tuple ot (Int,Double). We alias the values of the tuple to numberScores and totalScore\
 (sacraticing a one-liner tor readablility). We increment the number of scores by one and add the current score to the tota1scores received so far\
 The scoreMerger function takes two Scorecollectors adds the tota1number ot scores and the tota1scores together returned in a new tuple\
 We then cal1the combineByKey function passing our previously defined functions. \
We take the resulting RDD, scores, and cal1the collectAsMap function to get our results in the form of(name,(numberScores,totalScore)).\
To get our fina1result we cal1the map function on the scores RDD passing in the averagingFunction which simply calculates the average score and returns a tuple of(name,averageScore)\
 \
Calculating an average is a litte trickier compared to doing a count for the simple fact that counting is associative and commutative,we just sum al1values for each partiton and\
sum the partition values. But with averages, it's not that simple, an average ot averages is not the same as taking an average across al1numbers.But we can collect the total\
 number scores and tota1score per partition then divide the tota1overal1score by the number ot scores. \
\
-----------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 58 : you have been given code snippet.\
\
\
\
Va1a = sc.parallelize(List(1, 2, 1, 3), 1)\
\
Va1b = a.map((_, "b"))\
\
Va1c = a.map((_, "c"))\
\
\
\
Operation_xyz\
\
\
\
Write a correct code snippet for Operation_xyz which wil1produce below output.\
\
Output :\
\
Array[(Int, (Iterable[string], Iterable[String]))] = Array(\
\
(2,(ArrayBuffer(b),ArayBuffer(c))),\
\
(3,(ArrayBuffer(b),ArayBuffer(c))),\
\
(1,(ArrayBuffer(b, b),ArayBuffer(c, c)))\
\
)\
\
-----------------------------------------------------------------------------------------------------------------------------\
Solution : \
\
b.cogroup(c).collect \
\
cogroup [Pair], groupWith [Pair] \
\
A very powerfu1set of functions that allow grouping up to 3 key-value RDDS together using their keys. \
\
Another Thoughtworksple \
va1x = sc.parallelize(List((1, "apple"), (2, "banana"), (3, "orange"), (4, "kiwi")), 2) \
va1y = sc.parallelize(List((5"computer"), (1, "laptop"), (1, "desktop"), (4, "iPad")), 2) \
x.cogroup(y).collect \
\
Array[(lnt, (Iterable[String], Iterable[String]))] = Array( \
(4,(ArrayBuffer(kiwi),ArrayBuffer(iPad))), \
(2,(ArrayBuffer(banana),ArrayBuffer())), \
(3,(ArrayBuffer(orange),ArrayBuffer())), \
(1 desktop))), \
(5,(ArrayBuffer(),ArrayBuffer(computer)))) \
\
\
------------------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 59 : You have been given below code snippet.\
\
\
\
va1b = sc.parallelixe(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1))\
\
\
\
Operation_xyz\
\
\
\
Write a correct code snippet for Operation_xyz which wil1produce below output.\
\
scala.collection.map[Int,Long] = Map(5 -> 1, 8 -> 1, 3 -> 1, 6 -> 1, 1 -> 6, 2 -> 3, 4 -> 2, 7 -> 1)\
\
----------------------------------------------------------------------------------------------------------------------\
b.countByValue \
countByValue\
Returns a map that contains al1unique values of the RDD and their respective occurrence counts.(Warning:This operation wil1finally aggregate the information in a single reducer.)\
Listing Variants\
def countByValue():Map[t,Long] \
-----------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 60 : You have been given below code snippet.\
\
\
\
Va1a = sc.parallelize(1 to 10, 3)\
\
operation1\
\
b.collect\
\
\
\
Output 1\
\
Array[Int] = Array(2, 4, 6, 8, 10)\
\
\
\
Operation2\
\
Output 2\
\
Array[Int] = Array(1, 2, 3)\
\
\
\
Write a correct code snippet for operation1 and operation2 which wil1produce desired output, shown above.\
\
-------------------------------------------------------------------------------------------------------------\
Solution : \
\
va1b = a.filter(_ % 2 O) \
a.filter(_ < 4).collect \
\
filter \
Evaluates a boolean function for each data item of the RDD and puts the items for which the function returned true into the resulting RdD. \
When you provide a filter function, it must be able to handle al1data items contained in the RDD. Scala provides so-called partia1functions to dea1with mixed data-types(Tip: \
Partia1functions are very usefu1it you have some data which may be bad and you do not want to handle but for the good data (matching data) you want to apply some kind of map \
function. The following article is good. It teaches you about partia1functions in a very nice way and explains why case has to be used tor partia1functions:article) \
\
Thoughtworksples for mixed data without partia1functions \
\
va1b = sc.parallelize(1to 8) \
\
b.filter(_ < 4).collect \
\
res15: Array[lnt] = Array(l, 2, 3) \
\
va1a = sc.parallelize(List("cat", "horse", 4.0, 3.5, 2,"dog")) \
\
a.filter(_ < 4).collect \
\
error: value < is not a member ot Any\
\
--------------------------------------------------------------------------------------------------------------\
\
\
problem Scenario 61 : You have been given below code snippet.\
\
\
\
Va1a = sc,parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"))\
\
\
\
operation1\
\
\
\
Write a correct code snippet for operation1 which wil1produce desired output, shown below.\
\
Array[(Int, String)] = Array((4,lion), (7,panther), (3,dogcat), (5,tigereagle))\
\
--------------------------------------------------------------------------------------------------------------\
Solution : \
\
b.foldByKey("")(_ + _).collect \
fOldBYKey [Pam \
Very similar to fold, but performs the folding separately for each key of the RDD. This Function is only available it the RDD consists of two-component tuples.\
Listing Variants \
def foldByKey(zeroValue:V)(func:(V,V)=>V):RDD[(K,V)] \
def foldByKey(zeroValue:V, numpartitions:int)(func:(V, V)=> V):RDD[(K, V)] \
def foldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) => V):RDD[(K, V)] \
\
\
--------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 62 : You have been given below code snippet.\
\
\
\
va1pairRDD1 = sc.parallelixe(List( ("cat",2), ("cat",5), ("book",4),("cat", 12)))\
\
va1pairRDD2 = sc.parallelixe(List( ("cat",2), ("cup",5), ("mouse",4),("cat", 12)))\
\
\
\
operation1\
\
\
\
write a correct code snippet for operation1 which wil1produce desired output, shown below.\
\
Array[(String, (Option[Int], Option[Int]))] = Array((book,(Some(4),None)), (mouse,(None,Some(4))), (cup,(None,Some(5))), (cat,(Some(2), Some(2))), (cat,(Some(2),Some(12))), (cat,(Some(5),Some(2))), (cat,(Some(5),Some(12))), (cat,(Some(12),Some(2)), (cat,(some(12),Some(12)))\
\
-----------------------------------------------------------------------------------------------------------\
Solution : \
\
pairRDD1.fullouterJoin(pairRDD2).collect \
\
fullOuterJoin [Pair] \
\
Performs the ful1outer join between two paired RDDS. \
\
Listing Variants \
\
def fullOuterJion[W](other:RDD[(K,W)], numpartitions: Int):RDD[(K, (Option[V], Option[W]))] \
def fullOuterJion[W](other:RDD[(K,W)]):RDD[(K, (Option[V], Option[W]))]\
def fullOuterJion[W](other:RDD[(K,W)],partitioner:Partitioner):RDD[(K, (Option[V], Option[W]))]\
\
------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 63: You have been given below code snippet.\
\
\
\
Va1a = sc.parallelize(1 to 100, 3)\
\
\
\
operation 1\
\
\
\
Write a correct code snippet for operation1 which wil1produce desired output shown below\
\
Array[Array[Int]] = Array(Array( 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33),\
\
Array(34, 35, 36, 37, 38, 39. 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66),\
\
Array(67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79 , 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100))\
\
-----------------------------------------------------------------------------------------------------------------\
Solution : \
\
a.glom.collect \
\
glom \
\
Assembles an array that contains al1elements ot the partition and embeds it in an RDD. Each returned array contains the contents of one partition\
\
-----------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 64 : You have been given below code snippet.\
\
\
\
va1a = sc.parallelize( 1 to 9, 3)\
\
operation1\
\
\
\
Write a correct code snippet for operation1 which wil1produce desired output, shown below.\
\
Array[(String, Seq[Int])] = Array((even,ArrayBuffer(2, 4, 6, 8)), (odd, ArrayBuffer(1, 3, 5, 7, 9)))\
\
------------------------------------------------------------------------------------------------------------------------\
Solution:\
a.groupBy(x=>\{if(x%2==0)"even"else"odd"\}).collect\
\
------------------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenario 65 
\b0 \ulnone : You have been given below code snippet.\
\
vala = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2)\
valb = a.keyBy(_.length)\
\
\ul operation1\ulnone \
Write a correct code snippet for operation1 which wil1produce desired output, shown below\
Array[(Int, Seq[String])] = Array((4, ArrayBuffer(lion)), (6, ArrayBuffer(Spider)), (3, ArrayBuffer(dog, cat)), (5, ArrayBuffer(tiger, eagle)))\
\

\b \ul Solution : 
\b0 \ulnone \
b.groupByKey.collect \
groupByKey [Pair] \
Very similar to groupBy, but instead of supplying a function, the key-component of each pair wil1automatically be presented to the partitioner. \
Listing Variants \
def groupBYKey():RDD[(K, Iterable[V])] \
def groupByKey(numPartitions: Int): Iterable[V])] \
def groupByKey(partitioner: Partitioner):RDD Iterable[V])] \
-------------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenario 66 :
\b0 \ulnone  You have been given below code snippet.\
\
Valx = sc.parallelize(1 to 20)\
valy = sc.parallelize(1o to 30)\
operation1\
z.collect\
write a correct code snippet for operation1 which will produce desired output, shown below.\
Array[Int] = Array(16, 12, 20, 13, 17, 14, 18, 10, 19, 15, 11)\
\
\

\b \ul Solution:
\b0 \ulnone \
va1z = x.Intersection(y)\
Intersection : Returns the elements in the two RDDs which are the same.\
-------------------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenario 67 :
\b0 \ulnone  You have been given below code snippet.\
\
vala = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)\
valb = a.KeyBy(_.length)\
valc = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3)\
vald = c.KeyBy(_.length)\

\b \ul operation1
\b0 \ulnone \
Write a correct code snippet for operation1 which wil1produce desired output, shown below.\
Array[(Int, (String, String))] = Array((6,(Salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (6,(salmon,salmon)), (6,(salmon,rabbit)),\
(6,(salmon,turkey)), (3,(dog,dog)), (3,(dog,cat)), (3,(dog,gnu)), (3,(dog,bee)), (3,(rat,dog)), (3, (rat,cat)), (3,(rat,gnu)), (3,(rat,bee)))\
\
\

\b \ul Solution : 
\b0 \ulnone \
\
b.join(d).collect \
join [Pair] : Performs an inner join using two key-value RDDS. Please note that the keys must be generally comparable to make this work. \
keyBy : Constructs two-component tuples (key-value pairs) by applying a function on each data item. The result of the function becomes the key and the original\
data item becomes the value of the newly created tuples.\
------------------------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenario 68
\b0 \ulnone  : You have been given below code snippet.\
\
vala = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)\
valb = a.KeyBy(_.length)\
valc = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3)\
vald = c.KeyBy(_.length)\
\

\b \ul operation1 :- 
\b0 \ulnone \
Write a correct code snippet for operation1 which wil1produce desired output, shown below.\
Array[(Int, (String, option[String]))] = Array((6,(Salmon,some(salmon))), (6,(salmon,some(rabbit))), (6,(salmon,some(turkey))), (6,(salmon,some(salmon))), \
(6,(salmon,some(rabbit))),(6,(salmon,some(turkey))), (3,(dog,some(dog))), (3,(dog,some(cat))), (3,(dog,some(gnu))), (3,(dog,some(bee))), (3,(rat,some(dog))),\
(3, (rat,some(cat))), (3,(rat,some(gnu))), (3,(rat,some(bee))), (8,(elephant,None)))\
\

\b \ul Solution : 
\b0 \ulnone \
\
b.leftOuterJoin(d).collect \
lettOuterJoin [Pain : Performs an left outer join using two key-value RDDS. Please note that the keys must be generally comparable to make this work correctly\
keyBy : Constructs two-component tuples (key-value pairs) by applying a function on each data item. The result of the function becomes the key and the origina1\
data item becomes the value of the newly created tuples. \
----------------------------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenraio 69 
\b0 \ulnone : You have been given below code snippet.\
vala = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2)\
valb = a.map(x => (x.length, x))\
operation1\
Write a correct code snippet for operation1 which wil1produce desired output, shown below.\
Array[(Int, String)] = Array((3,xdogx), (5,xtigerx), (4,xlionx), (3,xcatx), (7,xpantherx), (5,xeaglex))\
\

\b \ul Solution : 
\b0 \ulnone \
\
b.mapValues("x" + _ +"x").collect\
mapValues [Pair] : Takes the values of a RDD that consists ot two-component tuples, and applies the provided function to transform each value.Then,it forms new two -component\
tuples using the key and the transformed value and stores them in a new RDD. \
\
--------------------------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenario 70
\b0 \ulnone  : You have been given below code snippet.\
\
vala = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2)\
valb = a.map(x => (x.length, x))\
operation1\
Write a correct code snippet for operation1 which wil1produce desired output, shown below.\
Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))\

\b \ul Solution :
\b0 \ulnone  \
b.reduceByKey(_ + _).collect \
reduceByKey [Pair] : This function provides the well-known reduce functionality in Spark. Please note that any function f you provide, \
should be commutative in order to generate reproducible results. \
--------------------------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenario 71 
\b0 \ulnone : You have been given below code snippet.\
vala = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)\
valb = a.KeyBy(_.length)\
valc = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3)\
vald = c.KeyBy(_.length)\
Operation :-\
Write a correct code snippet for operation1 which will produce desired output, shown below.\
Array[(Int, (Option[String], String))] = Array((6,(some(Salmon),salmon)), (6,(some(salmon),rabbit)), (6,(some(salmon),turkey)), (6,(some(salmon),salmon)),\
(6,(some(salmon),rabbit)), (6,(some(salmon),turkey)), (3,(some(dog),dog)), (3,(some(dog),cat)), (3,(some(dog),gnu)), (3,(some(dog),bee)), (3,(some(rat),dog)),\
(3, (some(rat),cat)), (3,(some(rat),gnu)), (3,(some(rat),bee)), (4,(None,wolf)), (4,(None,bear)))\
\

\b \ul Solution:
\b0 \ulnone \
b.rightOuterJion(d).collect\
rightOuterJoin[Pair]:Performs an right outer join using two key-value RDDS.Please note that the keys must be generally comparable to make this work correctly.\
\
--------------------------------------------------------------------------------------------------------------------\
\
Problem Scenario 72 : You have been given below code snippet.\
vala = sc.parallelize(List("dog", "cat", "owl", "gnu", "ant"), 2)\
valb = sc.parallelize(1 to a.count.toInt, 2)\
valc = a.zip(b)\
operation1\
Write a correct code snippet for operation1 which will produce desired output, shown below.\
Array[(String, Int)] = Array((owl,3), (gnu,4), (dog,1), (cat,2), (ant,5))\
\

\b \ul Solution : 
\b0 \ulnone \
c.sortByKey(false).collect \
sortByKey [Ordered] : This function sorts the input RDD's data and stores it in a new RDD. The output RDD is a shuffled RDD because it stores data that is output by a reducer \
which has been shuffled. The implementation of this function is actually very clever. First, it uses a range partitioner to partition the data in ranges.within the shuffled RDD. \
Then it sorts these ranges individually with map partitions using standard sort mechanisms. \
\
---------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 73 :
\b0 \ulnone  You have been given below code snippet.\
va1a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2)\
va1b = a.KeyBy(_.lenth)\
va1b = sc.parallelize(List("ant", "falcon", "squid"), 2)\
va1c = c.KeyBy(_.length\
operation1\
Write a correct code snippet for operation1 which will produce desired output, shown below.\
Array[(Int, String)] = Array((4,lion))\
\

\b \ul Solution :
\b0 \ulnone \
b.subtractByKey(d).collect\
subtractByKey[Pair]:Very similar to subtract,but instead of supplying a function,the key-component of each pair will be automatically used as criterion for removing items from the first RDD.\
------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 74 :
\b0 \ulnone  You have been given below code snippet.\
lines = sc.parallelize(['Its fun to have fun,','but you have to know how.'])\
r1 = lines.map( lambda x: x.replace(',',',').replace('.',' ').replace('-',' ').lower())\
r2 =r1.flatMap(lambda x: x.split())\
r3 = r2.map(lambda x: (x, 1))\
operation1\
r5 = r4.map(lambda x:(x[1],x[0]))\
r6 = r5.sortByKey(ascending=False)\
r6.take(20)\
Write a correct code snippet for operation1 which wil1produce desired output, shown below.\
[(2, 'fun'), (2, 'to'), (2, 'have'), (1, 'its'), (1, 'know'), (1, 'how'), (1, 'you'), (1, 'but')]\
----------------------------------------------------------------------------------------------------------\
Solution:\
r4=r3.reduceByKey(lambda x,y:x+y)\
--------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenario 75 
\b0 \ulnone : You have given a file as below.\
\
spark75/file1.txt\
\
Apache Hadoop is an open-source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. Al1the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework\
The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File System (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers packaged code for nodes to process in parallel based on the data that needs to be processed.\
This approach takes advantage of data locality nodes manipulating the data they have access to to allow the dataset to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are distributed via high-speed networking\
For a slightly more complicated task, lets look into splitting up sentences from our documents into word bigrams. A bigram is pair of successive tokens in some sequence. We wil1look at building bigrams from the sequences of words in each sentence, and then try to find the most frequently occurring ones.\
The first problem is that values in each partition of our initial RDD describe lines from the file rather than sentences. Sentences may be split over multiple lines. The glom() RDD method is used to create a single entry for each document containing the list of al1lines, we can then join the lines up, then re split them into sentences using "." as the separator, using flatMap so that every object in our RDD is now a sentence.\
A bigram is pair of successive tokens in some sequence, please build bigrams from the sequences of words in each sentence, and then try to find the most frequently occurring ones.\
\

\b \ul Solution:
\b0 \ulnone \
Step 1 : Create al1three files in hdfs (We wil1do using Hue). However, you can first create in loca1filesystem and then upload it to hdfs. \
Step 2 : The first problem is that values in each partition of our initia1RDD describe lines from the file rather than sentences. \
Sentences may be split over multiple lines. \
The glom() RDD method is used to create a single entry for each document containing the list ot al1lines, we can then join the lines up, \
then resplit them into sentences using "." as the separator, using flatMap so that every object in our RDD is now a sentence. \
sentences = sc.textFile("spark75/file1 .txt") \\ \
.glom() \\ \
.map(lambda x: " ".join(x)) \\ \
.flatMap(lambda x: x.split(".")) \
Step 3 : Now we have isolated each sentence we can split it into a list of words and extract the word bigrams from it. Our new RDD contains tuples \
containing the word bigram (itself a tuple containing the first and second word) as the first value and the number 1 as the second value. \
bigrams = sentences.map(lambda x:x.split()) \\ \
.flatMap(lambda x:[((x[i],x[i+1]),1) tor i in range(O,len(x)-1)]) \
 Step 4 : Finally we can apply the same reduceByKey and sort steps that we used in the wordcount Thoughtworksple, to count up the bigrams and sort \
them in order ot descending frequency. In reduceByKey the key is not an individua1word but a bigram. \
freq_bigrams = bigrams.reduceByKey(lambda x,y:x+y) \\ \
.map(lambda x:(x[1],x[0]))\\ \
.sortByKey(False) \
freq_bigrams.take(10) \
-----------------------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenario 76 
\b0 \ulnone : You have been given MySQ1DB with following details. Using Sqoop export al1these tables in Avro file format.\
\
Once al1the files are exported, create external table in hive using these Avro files. Also make sure data is loaded in hive tables as well.\
user=retail_dba password=cloudera database-retail_db jdbcURL= jdbc:mysql://quickstart:3306/retail_db\
And following details must be used\
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 
\b0 \ulnone : Export tables in Avro file format from MySQ1RDBMS (It wil1create avro files correspond to \'e9ach table in (/user/hive/warehouse/retail_dtage1.db) \
sqoop import-all-tables -m1--connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --as-avrodatatile  --warehouse-dir=/user/hive/warehouse/retai1stagel.db \

\b \ul step 2 :
\b0 \ulnone  It would have created *.avsc files on loca1machine from where, you have executed sqoop import as below. (These al1are avro schema files)\
 ls -ltr *.avsc \
-rw-rw-r\'971 cloudera cloudera 409 Feb 16 11:54 sqoop_import_departments.avsc \
-rw-rw-r\'971 cloudera cloudera 541 Feb 16 11:54 sqoop_import_categories.avsc \
-rw-rw r\'971 cloudera cloudera 1324 Feb 16 11:54 sqoop_import_customers.avsc \
-rw-rw r\'971 cloudera cloudera 980 Feb 16 11:55 sqoop_import_order_items.avsc \
-rw-rw r-1 cloudera cloudera 632 Feb 16 11:56 sqoop_import_orders.avsc \
-rw-rw-r\'971 cloudera cloudera 922 Feb 16 11:56 sqoop_import_products.avsc \

\b \ul Step 3 :
\b0 \ulnone  Actual data will be exported in avro format at (in hdfs) /user/hive/warehouse/retail_stage1.db \
hadoop fs -ls /user/hive/warehouse/retail_stage1.db \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:14 /user/hive/warehouse/retail_stage1.db/categories \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:14 /user/hive/warehouse/retail_stage1.db/customer \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:15 /user/hive/warehouse/retail_stage1.db/departments \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:16 /user/hive/warehouse/retail_stage1.db/ order_items\
drwxr-xr-x - cloudera hive 0 2016-03-13 05:16 /user/hive/warehouse/retail_stage1.db/orders \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:17 /user/hive/warehouse/retail_stage1.db/products \
We can check data in one of the avro file \
hadoop fs -cat /user/hive/warehouse/retail_stage1.db/EMPLOYEE/part-m-OOOOO.avro\

\b \ul Step 4 : 
\b0 \ulnone Now copy schema files from local machine to /user/cloudera/retail_stage1(hdfs) \
Create a directory on hdfs \
hadoop fs -mkdir/user/cloudera/retail_stage1\
Put a1avsc file from local to hdfs \
hadoop fs -put *.avsc/user/cloudera/retail_stage1\
Check files are copied or not \
hadoop fs -ls /user/cloudera/retail_stage1\

\b \ul Step 5 
\b0 \ulnone : Now go to hive \
hive \
show databases; \
Create a database\
create database retail_stagel; \
show databases; \

\b \ul Step 6 : 
\b0 \ulnone Use this database and import al1the avro file , while creating tables. \
use retail_stagel; \
show tables; \'97 it will result in no tables. \

\b \ul Step 7 : 
\b0 \ulnone Create external tables. \
CREATE EXTERNAL TABLE categories \
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.Avroseroe' \
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.i0.avro.AvrocontainennputFormat' \
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' \
LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/categories' \
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera//user/cloudera/retail_stagel/sqoop_import_categories.avsc'); \
CREATE EXTERNAL TABLE customers \
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.Avroseroe' \
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvrocontainennputFormat' \
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' \
LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/customers' \
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera//user/cloudera/retail_stagel/sqoop_import_departments.avsc'); \
CREATE EXTERNALTABLE departments \
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroserDe' \
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.i0.avro.AvrocontainennputFormat' \
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' \
LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/departments' \
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera//user/cloudera/retail_stagel/sqoop_import_departments.avsc'); \
CREATE EXTERNALTABLE orders \
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroserDe' \
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvrocontainennputFormat' \
OUTPUTTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' \
LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/orders' \
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera//user/cloudera/retail_stagel/sqoop_import_orders.avsc');\
CREATE EXTERNAL TABLE orders_items \
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroserDe' \
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvrocontainennputFormat' \
OUTPUTTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' \
LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/orders_items' \
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera//user/cloudera/retail_stagel/sqoop_import_orders_items.avsc'); \
CREATE EXTERNAL TABLE products \
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroserDe' \
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.i0.avro.AvrocontainennputFormat' \
OUTPUTTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' \
LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/products' \
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera//user/cloudera/retail_stagel/sqoop_import_products.avsc'); \

\b \ul step 8:
\b0 \ulnone Check the content in each table\
Select * from categories;\
Select * from customers;\
Select * from departments;\
Select * from orders;\
Select * from order_items;\
Select * from products;\
-------------------------------------------------------------------------------------------------------------------------------------\
\

\b \ul Problem Scenario 77
\b0 \ulnone  : You have been given MySQ1DB with following details. Using Sqoop export al1these tables in Avro file format.\
\
Once all the files are exported, create external table in hive using these Avro files. Also make sure data is loaded in hive tables as well.\
user=retail_dba password=cloudera database-retail_db jdbcURL= jdbc:mysql://quickstart:3306/retail_db\
while creating tables, you have to make sure that data "SSTORED AS AVRO" and you do not have to use explicitly SERDE and INPUT and OUTPUT Formats.\

\b \ul Solution:
\b0 \ulnone \
\

\b \ul Step 1 :
\b0 \ulnone  Export tables in Avro file format from MySQL RDBMS (It will create avro files correspond to each table in (/user/hive/warehouse/retail_stage1.db) \
sqoop import-all-tables -m1 --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera --as-avrodatafile --warehouse-dir=/user/hive/warehouse/retail_stage1.db \

\b \ul step 2 : 
\b0 \ulnone It would have created *.avsc files on local machine from where, you have executed sqoop import as below. (These al1are avro schema files)\
ls -ltr *.avsc \
-rw-rw r--1 cloudera cloudera 409 Feb 16 11:54 sqoop_import_departments.avsc \
-rw-rw r--1 cloudera cloudera 541 Feb 16 11:54 sqoop_import_categories.avsc \
-rw-rw r-- 1 cloudera cloudera 1324 Feb 16 11:54 sqoop_import_customers.avsc \
-rw-rw r--1 cloudera cloudera 980 Feb 16 11:55 sqoop_import_order_items.avsc \
-rw-rw-r--1 cloudera cloudera 632 Feb 16 11:56 sqoop_import_orders.avsc \
-rw-rw-r\'97 1 cloudera cloudera 922 Feb 16 11:56 sqoop_import_products.avsc  \

\b \ul Step 3 
\b0 \ulnone : Actual data will be exported in avro format at (in hdfs) /user/hive/warehouse/retail_stage1.db \
hadoop fs -ls /user/"ive/warehouse/retai1stagel.db \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:14 /user/hive/warehouse/retail_stagel.db/categories \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:14 luser/hive/warehouse/retail_stagel.db/customers \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:15 luser/hive/warehouse/retail_stagel.db/departments \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:16 luser/hive/warehouse/retail_stagel.db/order items \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:16 luser/hive/warehouse/retail_stagel.db/orders \
drwxr-xr-x - cloudera hive 0 2016-03-13 05:17 luser/hive/warehouse/retail_stagel.db/products\
We can check data in one of the avro file\
hadoop fs -cat/user/hive/warehouse/retail_stage1.db/EMPLOYEE/part-m-00000.avro \

\b \ul Step 4 : 
\b0 \ulnone Now copy schema files from local machine to /user/cloudera/retail_stage1(hdfs) \
Create a directory on hdfs \
hadoop fs -mkdir/user/cloudera/retail_stage1\
Put all avsc file from local to hdfs \
hadoop fs -put *.avsc /user/cloudera/retail_stage1\
Check files are copied or not \
hadoop fs -ls /user/cloudera/retail_stage1\

\b \ul Step 5 
\b0 \ulnone : Now go to hive \
hive \
show databases; \
Create a database \
create database retailstage1; \
show databases; - \

\b \ul Step 6 : 
\b0 \ulnone use this database and import al1the avro file , while creating tables. \
use retail_stage1; \
show tables; --it will result in no tables. \

\b \ul Step 7 :
\b0 \ulnone  Create external tables. \
create external table categories1\
stored as avro \
location 'hdfs:///user/hive/warehouse/retail_stage1.db/categories' \
tblproperties ('avro.schema.url'='hdfs://quickstart.cloudera/user/cloudera/retail_stage1/sqoop_import_categories.avsc');\
create external table customers1\
stored as avro \
location 'hdfs:///user/hive/warehouse/retail_stage1.db/customers' \
tblproperties ('avro.schema.url'='hdfs://quickstart.cloudera/user/cloudera/retail_stage1/sqoop_import_customers.avsc'); \
create external table departments1\
stored as avro \
location 'hdfs:///user/hive/warehouse/retail_stagel.db/departments' \
tblproperties ('avro.schema.url'='hdts://quickstart.cloudera/user/cloudera/retail_stage1/sqoop_import_departments.avsc' );\
create external table orders1\
stored as avro \
location 'hdfs:///user/hive/warehouse/retail_stage1.db/orders' \
tblproperties ('avro.schema.url'='hdts://quickstart.cloudera/user/cloudera/retail_stage1/sqoop_import_orders.avsc');\
create external table order_items1\
stored as avro \
location 'hdfs:///user/hive/warehouse/retail_stage1.db/order_items' \
tblproperties ('avro.schema.url'='hdts://quickstart.cloudera/user/cloudera/retail_stage1/sqoop_import_order_items.avsc'); \
create external table products1\
stored as avro \
location 'hdfs:///user/hive/warehouse/retail_stage1.db/products' \
tblproperties ('avro.schema.url'='hdfs://quickstart.cloudera/user/cloudera/retail_stage1/sqoop_import_products.avsc');  \

\b \ul step 8 : 
\b0 \ulnone Check the content in each table. \
Select * from categories1; \
Select * from customers1; \
Select * from departments1; \
Select * from orders1; \
Select * from order items1; \
Select * from products; \
------------------------------------------------------------------------------------------------------------------------------------\

\b \ul Program Scenario 78 
\b0 \ulnone : You have been given below Avro schema definition, this is created when we exported MySQ1department table\
using sqoop as Avro format\
\{\
  "type" : "record",\
  "name" : "sqoop_import_departments",\
  "doc" : "Sqoop import of departments",\
  "fields" : [ \{\
    "name" : "department_id",\
    "type" : [ "int", "null" ],\
    "columnName" : "department_id",\
    "sqlType" : "4"\
  \}, \{\
    "name" : "department_name",\
    "type" : [ "string", "null" ],\
    "columnName" : "department_name",\
    "sqlType" : "12"\
  \} ],\
  "tableName" : "departments"\
\}\
\
And data file location in hdfs as below.\
user/hive/warehouse/retail_stage1.db/departments\
Create a table in hive schema retail_stage1 named departments2 using above schema and data file.\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 
\b0 \ulnone : Inside hive terminal\
use retail_stagel; \

\b \ul Step 2 
\b0 \ulnone : We have to use "avro.schema.literal" as table properties to refer avro schema \
CREATE EXTERNA1TABLE departments2 \
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroserDe' \
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvrocontainennputFormat' \
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' \
LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/departments' \
TBLPROPERTIES ('avro.schema.literal'='\{\
"type" : "record", \
"name" : "sqoop_import_departments", \
"doc" : "Sqoop import of departments", \
"fields" : [ \{ \
"name" : "department_id", \
"type" : [ "int", "null" l, \
"columnName" : "department_id", \
"sqlType" : "4" \
\},\{\
"name" : "department_name", \
"type" : [ "string", "null" l, \
"columnName" : "department_name", \
"sqlType" : "12" \
\}], \
"tableName" : "departments" \
\}');\

\b \ul step 3 : 
\b0 \ulnone \
Check the loaded data \
select * from departments2; \
**************************************************************************************************************************************\

\b \ul Problem Scenario 79 : 
\b0 \ulnone You have been given below information\
Avro data file location : hdfs:///user/hive/warhouse/retail_stage1.db/categories\
Avro schema file location : hdfs://quickstart.cloudera/user/cloudera/retail_stage1/sqoop_import_categories.avsc\
Now using the Impala create a table in "retail_stage1" schema and table name as categories2\
\

\b \ul Solution : 
\b0 \ulnone \
Step 1 : Start Impala shell\
impala-shel1\
show databases; \
use retail_stage1; \

\b \ul Step 2 : 
\b0 \ulnone Create table using below DD1\
CREATE EXTERNA1TABLE categories2 \
STORED AS AVRO\
LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/categories' \
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/cloudera/retail_stagel/sqoop_import_categories.avsc'); \

\b \ul Step 3 
\b0 \ulnone : Check tables and data \
select * from categories2; \
\
**************************************************************************************************************************************\
\

\b \ul Problem Scenario 80 
\b0 \ulnone : You have been given below information\
Avro schema as below\
\{\
  "type" : "record",\
  "name" : "orders_partition",\
  "doc" : "Hive partitioned table schema ",\
  "fields" : [ \{\
    "name" : "order_id",\
    "type" : [ "int", "null" ]\
  \}, \{\
    "name" : "order_date",\
    "type" : [ "long", "null" ]\
  \}, \{\
    "name" : "order_customer_id",\
    "type" : [ "int", "null" ]\
  \}, \{\
    "name" : "order_status",\
    "type" : [ "string", "null" ]\
  \} ],\
  "tableName" : "orders_partition"\
\}\
\
Perform following activities \
1. Create a managed Hive table which is partitioned by order_month (data type is string) , \
and location of the data should be LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/orders_partition'. Columns in table wil1be as below \
order_id int, \
order_date bigint, \
order customer id int, \
order_status string \
2. Once the table is created , create a manual partition like (order_month='2014-02') \
3. Now load the data from select * from orders2 table, which is available in "retail_stagel" . However, while loading data make sure you load only data from '2014-02" month. \
\

\b \ul Solution : 
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create hive managed table, also provide name ot the partition column , location ot the table storage and avro schema as literal. \
use retail_stagel; \
CREATE TABLE orders_partition ( \
order_id int, \
order_date bigint, \
order_customer_id int, \
order_status string \
)\
PARTITIONED BY (order_month string) \
STORED AS AVRO \
LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/orders_partition' \
TELPROPERTIES('avro.schema.literal'='\{\
"type" : "record", \
"name" : "orders _ partition", \
"doc" : "Hive partitioned table schema ' \
"fields" : [ \{ \
"name" : "order_id", \
"type" : [ "int", "null" ] \
\},\{\
"name" : "order_date", \
"type" : [ "long", "null" ] \
\},\{\
"name" : "order_customer_id", \
"type" : [ "int", "null" ] \
\},\{\
"name" : "order_status", \
"type" : [ "string", "null" ] \
\}'],\
"tableName" : "orders_partition" \
\}');\
Check whether table is created or not \
show tables; \

\b \ul Step 2 :
\b0 \ulnone  Create manual partition on the table \
alter table orders-_partition add partition (order_month='2014-02'); \

\b \ul Step 3 :
\b0 \ulnone  Load data from orders1table \
insert into table orders _ partition partition (order_month='2014-02') \
select * from orders where from_unixtime(cast(substr(order_date, 1, 10) as int)) like '2014-02%'; \

\b \ul Step 4 : 
\b0 \ulnone Now check the loaded data \
select * from orders_partition; \
Check content created at partition location \
hadoop fs -ls /user/hive/warehouse/retail_stage1.db/departments \
hadoop fs -ls hdfs:///user/hive/warehouse/retail_stage1.db/orders_partition/order_month=2014-02 \
---------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 81 : 
\b0 \ulnone You have been given below information\
Avro schema as below\
\{\
  "type" : "record",\
  "name" : "orders_partition1",\
  "doc" : "Hive partitioned table schema ",\
  "fields" : [ \{\
    "name" : "order_id",\
    "type" : [ "int", "null" ]\
  \}, \{\
    "name" : "order_date",\
    "type" : [ "long", "null" ]\
  \}, \{\
    "name" : "order_customer_id",\
    "type" : [ "int", "null" ]\
  \}, \{\
    "name" : "order_status",\
    "type" : [ "string", "null" ]\
  \} ],\
  "tableName" : "orders_partition1"\
\}\
\
Perform following activities\
1. Create a managed Hive table which is partitioned by order_month (data type is string e.g. '2015-01'),\
and location of the data should be LOCATION 'hdfs://user/hive/warehouse/retail_stage1.db/orders_partition1'. Cloumns in table wil1be as below\
order_id int,\
order_date bigint,\
order_customer_id int\
order_status string\
2. Once the table is created, use dynamic partitioning to load the data from entire orders1 table\
-------------------------------------------------------------------------------------------------------------------------\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create hive managed table, also provide name of the partition column , location of the table storage and avro schema as literal. \
use retail_stage; \
CREATE TABLE orders_partition1( \
order_id int, \
order_date bigint, \
order_customer_id int, \
order_status string \
)\
PARTITIONED BY (order_month string) \
STORED AS AVRO \
LOCATION 'hdfs:///user/hive/warehouse/retail_stage1.db/orders_partition1' \
TBLPROPERTIES('avro.schema.literal'='\{ \
"type" : "record", \
"name" : "orders_partition1\'94, \
"doc" : "Hive partitioned table schema ", \
"fields" : [ \{ \
"name" : "order_id", \
"type" : ["int", "null" ] \
\},\{\
"name" : "order_date", \
"type" : [ "long", "null" ] \
\},\{\
"name" : "order_customer_id", \
"type" : [ "int", "null" ] \
\},\{\
"name" : "order_status", \
"type" : [ "string", "null" ] \
\}],\
"tableName" : "orders_partition1\'94 \
\}');\
Check whether table is created or not \
show tables; \

\b \ul Step 2 :
\b0 \ulnone  Load data using dynamic partitioning \
set hive.exec.dynamic.partition.mode=nonstrict; \
insert into table orders_partition1 partition (order_month) \
select order_id, order_date, order_customer_id, order_status, \
substr(from_unixtime(cast(substr(order_date, 1, 10) as int)), 1, 7) order_month from ordersl; \

\b \ul Step 3 :
\b0 \ulnone  Check partition has been created or not \
hadoop fs -ls /user/hive/warehouse/retail_stage1.db/orders_partitionl/* \
select * from orders_partition1; \
-------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 82:
\b0 \ulnone  You have been given a csv file with following column names\
first_name,last_name,address,country,city,state,pincode,home,office,email,website\
Binita,jain,Kisan Road,IN,Bikaner,RJ,343806,0381749123,0244458665290,Binita.jain@jain.com,http://www.hadoopThoughtworks.com\
Rakesh,Suthar,Firozabad Gali,IN,Ahmedabad,RJ,434567,04499973366,022497622620,Rakesh.suthar@hotmail.com,http://www.hadoopThoughtworks.com\
Do the following Activities\
1.Create a managed table (Write DDL) in hive, named as "USER_PART_BY_COUNTRY"table, and must be partitioned by country and state column.\
2.Now create a data file using above data and load in a partition as country = 'IN', state = 'RJ'\
3.Create another table name "HadoopThoughtworks_USER_COUNTRY_SEQ', partitions as country, state and data stored as SEQUENCEFILE.\
4.Load data in HadoopThoughtworks_USER_COUNTRY_SEQ from USER_PART_BY_COUNTRY and dynamic partitioning must be used while loading data.\
\

\b \ul Solution : 
\b0 \ulnone \

\b \ul Step 1 : 
\b0 \ulnone Create table in hive \
use retail_stage; \
CREATE TABLE HadoopThoughtworks_USER_COUNTRY(\
firstname VARCHAR(64), \
lastname VARCHAR(64), \
address STRING, \
city VARCHAR(64), \
pincode STRING, \
home VARCHAR(64), \
office STRING, \
emai1STRING, \
website STRING \
)\
PARTITIONED BY (country VARCHAR(64), state VARCHAR(64)) \
ROW FORMAT DELIMITED \
FIELDS TERMINATED BY ','\
Stored as textfile;\

\b \ul Step 2 :
\b0 \ulnone  Verity the table \
DESCRIBE FORMATTED HadoopThoughtworks_USER_COUNTRY; \

\b \ul Step 3 :
\b0 \ulnone  Create file in hdfs using Hue at \
/user/cloudera/problem82/data.txt\

\b \ul Step 4 :
\b0 \ulnone  Load data in partition \
LOAD DATA INPATH '/user/cloudera/pr0blem82/data.txt' \
INTO TABLE HadoopThoughtworks_USER_COUNTRY \
PARTITION (country = 'US', state = 'CA'); \

\b \ul Step 5 :
\b0 \ulnone  Verify data has been loaded or not. \
SELECT* from HadoopThoughtworks_USER_COUNTRY; \

\b \ul step 6 : 
\b0 \ulnone create table HadoopThoughtworks_USER_COUNTRY_SEQ \
CREATE TABLE HadoopThoughtworks_USER_COUNTRY_SEQ(\
firstname VARCHAR(64), \
lastname VARCHAR(64), \
address STRING, \
City VARCHAR(64), \
pincode STRING, \
home VARCHAR(64), \
office STRING, \
emai1STRING, \
website STRING \
)\
PARTITIONED BY (country VARCHAR(64), state VARCHAR(64)) \
STORED AS SEQUENCEFILE; \

\b \ul Step 7 :
\b0 \ulnone  Load data by enabling dynamic partitioning mode. \
set hive.exec.dynamic.partition.mode=nonstrict; \
INSERT INTO TABLE HadoopThoughtworks_USER_COUNTRY_SEQ\
PARTITION (country, state) \
SELECT firstname, \
lastname, \
address, \
city, \
pincode, \
home, \
office, \
email, \
website, \
country, \
state \
FROM HadoopThoughtworks_USER_COUNTRY; \

\b \ul Step 8 :
\b0 \ulnone  Verity data has been loaded or not. \
SELECT* from HadoopThoughtworks_USER_COUNTRY_SEQ; \
-------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 83 
\b0 \ulnone : You have been given below information\
\
Avro schema as below\
\{\
  "type" : "record",\
  "name" : "orders_partition4",\
  "doc" : "Hive partitioned table schema ",\
  "fields" : [ \{\
    "name" : "order_id",\
    "type" : [ "int", "null" ]\
  \}, \{\
    "name" : "order_date",\
    "type" : [ "long", "null" ]\
  \}, \{\
    "name" : "order_customer_id",\
    "type" : [ "int", "null" ]\
  \}, \{\
    "name" : "order_status",\
    "type" : [ "string", "null" ]\
  \} ],\
  "tableName" : "orders_partition4"\
\}\
\
1.Create a managed hive table based on above schema and two additional columns as below\
order_value with default value as -9999 (This column can not be null)\
order_description with default value as "Not Defined" ( This column can store null)\
2. While creating table use the data from below location.\
hdfs://user/hive/warehouse/retail_stage.db/orders\
\

\b \ul Solution:
\b0 \ulnone \
\

\b \ul Step 1 :
\b0 \ulnone  Create a schema file named order.avsc locally and modify it for two additional column as below also add the default value for these new columns.\
"type" : "record", \
"name" : "orders _ partition1\'94, \
"doc" : "Hive partitioned table schema ", \
"fields" : [ \{ \
"name" : "order id", \
"type" : [ "int", "null" ] \
\},\{\
 "name" : "order_date", \
"type" : [ "long", "null" ] \
\},\{\
"name" : "order_customer_id", \
"type" : [ "int", "null" ] \
\},\{\
"name" : "order_status" \
"type" : [ "string", "null" ] \
\},\{\
"default" : -9999 \
"name" : "order_value", \
"type" :"int",\
"default" : -9999\
\},\{ \
"name":"order_description",\
"type":["string","null"],\
"default":"Not Defined"\
\}]\
"tableName" : "orders_partition2" \
\}\
\

\b \ul Step 2 
\b0 \ulnone : Create an avsc file in hdfs at below location (You can use either hue or create file locally and upload to hdfs) \
hdfs://quickstart.cloudera/user/cloudera/retail_stage1/sqoop_import_orders2.avsc \

\b \ul step 3 : 
\b0 \ulnone \
Create a managed table \
CREATE TABLE orders_partition4 ( \
order_id int, \
order_date bigint, \
order_customer_id int, \
order_status string, \
order_value int, \
order_description string \
)\
STORED AS AVRO \
LOCATION 'hdfs://///user/hive/warehouse/retail_stage.db/orders' \
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/cloudera/retail_stage1/sqoop_import_orders2.avsc'); \

\b \ul Step 4 
\b0 \ulnone : Check the tables; \
select * from orders_partition4; \
describe orders_partition4; \
-----------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 84
\b0 \ulnone  : Write down a Spark Application using Python, In which it read a file "Content.txt" (On hdfs) with following content.\
And filter out the word which is less than 2 characters and ignore al1empty lines.\
once done store the filtered data in a directory called "problem84" (On hdfs)\
Content.txt\
Hello this is HadoopThoughtworks.com \
Apache Spark Training\
This is Spark Learning Session\
Spark is faster than MapReduce\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 :
\b0 \ulnone  Create an application with following code and store it in problem84.py \
\ul # Import SparkContext and SparkConf \ulnone \
from pyspark import SparkContext, SparkConf \
\ul # Create configuration object and set App name \ulnone \
conf = SparkConf().setAppName("CCA 175 Problem 84") \
sc = SparkContext(conf=conf) \
\ul #load data trom hdfs \ulnone \
contentRDD = sc.textFile("Content.txt") \
\ul #filter out non-empty lines \ulnone \
nonempty_lines = contentRDD.tilter(lambda x: len(x) > O) \
#Split line based on space \
words = nonempty_lines.flatMap(lambda x: x.split(' ')) \
#filter out all  letter words \
finalRDD = words.filter(lambda x: len(x) > 2) \
for word in finalRDD.collect(): \
print(word) \
#Save fina1data\
finalRDD.saveAsTextFile("problem84") \
Step 2 : Submit this application \
spark-submit --master yarn problem84.py \
------------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 85 :
\b0 \ulnone  Write down a Spark Application using Python, In which it read a file "Content.txt" (On hdfs) with following content.\
Do the word count and save the results in a directory called "problems85" (On hdfs)\
Content.txt\
Hello this is HadoopThoughtworks.com \
This is HadoopThoughtworks.com\
Apache Spark Training\
This is Spark Learning Session\
Spark is faster than MapReduce\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Create an application with following code and store it in problem84.py 
\b0 \ulnone \
# Import SparkContext and SparkConf\
from pyspark import SparkContext, SparkConf\
\ul # Create configuration object and set App name\ulnone  \
conf = SparkConf().setAppName("CCA 175 Problem 85") \
sc = SparkContext(conf=conf) \
\ul #load data from hdfs \ulnone \
contentRDD = sc.textFile("Content.txt") \
\ul #filter out non-empty lines \ulnone \
nonempty_lines = contentRDD.tilter(lambda x: len(x) > O) \
\ul #Split line based on space \ulnone \
words = nonempty_lines.flatMap(lambda x: x.split(' ')) \
\ul #Do the word count \ulnone \
wordcounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y) .map(lambda x: (x[l], x[O])).sortByKey(False) \
for word in wordcounts.collect(): \
print(word) \
\ul #Save fina1data \ulnone \
wordcounts.saveAsTextFile("problem85") \

\b \ul Step2 : Submit this application
\b0 \ulnone  \
spark-submit --master yarn problem85.py \
--------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 86
\b0 \ulnone : Write down a Spark Script using Python, In which it read a file "content.txt" (On hdfs) with following content.\
After that split each row as (key, value), where key is first word in line and entire line as value.\
Filter out the empty lines. and save this key value in "problem86" as Sequence file (On hdfs)\
Part 2: Save as sequence file, where key as nul1and entire line as value, Read back the stored sequence files.\
\
Content.txt\
Hello this is HadoopThoughtworks.com \
This is HadoopThoughtworks\
Apache Spark Training\
This is Spark Learning Session\
Spark is faster than MapReduce\

\b \ul Step 1 
\b0 \ulnone . \
# Import SparkContext and SparkConf \
from pyspark import SparkContext, SparkConf \

\b \ul step 2 :
\b0 \ulnone  \
#load data from hdfs \
contentRDD = sc.textFile("Content.txt") \

\b \ul step 3 : 
\b0 \ulnone \
#filter out non-empty lines \
nonempty_lines = contentRDD.filter(lambda x: len(x) > O) \

\b \ul Step 4: 
\b0 \ulnone \
#Split line based on space (Remember : It is mandatory to convert is in tuple) \
words = nonempty_lines.map(lambda x: tuple(x.split(' ',1))) \
words. saveAsSequenceFile("problem86") \

\b \ul Step 5: Check contents in directory problem86 
\b0 \ulnone \
hdfs dfs -cat problem86/part* \

\b \ul Step 6 : Create key, value pair (where key is null)
\b0 \ulnone  \
nonempty_lines.map(lambda line: (None,line)).saveAsSequenceFile("problem86_1")\

\b \ul Step 7 : Reading back the sequence file data using spark. 
\b0 \ulnone \
seqRDD = sc.sequenceFile("problem86_1 ") \
Step 8 : Print the content to validate the same. \
for line in seqRDD.collect(): \
print(line) \
--------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 87
\b0 \ulnone  : You have been given a table named "employee2" with following detail.\
First_name string\
last_name string\
Write a spark script in python which read this table and print al1the rows and individua1column values.\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Import statements for HiveContext 
\b0 \ulnone \
from pyspark.sq1import HiveContext \

\b \ul Step 2 : Create sqlContext 
\b0 \ulnone \
sqlContext = HiveContext(sc) \

\b \ul Step 3 : Query hiv
\b0 \ulnone e \
employee2 = sqlContext.sql("select * from employee2") \

\b \ul Step 4 : Now prints the data 
\b0 \ulnone \
for row in employee2.collect(): \
print(row) \

\b \ul Step 5 : Print specific column
\b0 \ulnone  \
for row in employee2.collect(): \
print(row.first_name) \
--------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 88 
\b0 \ulnone : You have been given data in json format as below.\
\{"first_name":"Ankit", "last_name":"Jain"\}\
\{"first_name":"Amir", "last_name":"Khan"\}\
\{"first_name":"Rajesh", "last_name":"Khanna"\}\
\{"first_name":"Priynka", "last_name":"Chopra"\}\
\{"first_name":"Kareena", "last_name":"Kapoor"\}\
\{"first_name":"Lokesh", "last_name":"Yadav"\}\
Do the following activity\
\
1. create employee.json file locally.\
2. Load this file on hdfs\
3. Register this data as a temp table in spark using Python.\
4. Write select query and print this data\
5. Now save back this selected data in Json format.\
\
Solution:\

\b \ul Step 1:create employee.json file locally.
\b0 \ulnone \
vi employee.json\
(press insert) \
past the content. \

\b \ul Step 2 : Upload this file to hdfs, default location
\b0 \ulnone  \
hadoop fs -put employee.json \

\b \ul Step 3 : Write spark script
\b0 \ulnone  \

\b \ul #lmport SQLContext 
\b0 \ulnone \
from pyspark import SQLContext \
\ul #Create instance of SQLContext\ulnone  \
sqlContext = SQLContext(sc) \
\ul #Load json file \ulnone \
employee = sqlContext.jsonFile("employee.json") \
\ul #Register RDD as a temp table \ulnone \
employee. registerTempTable("EmployeeTab") \
\ul #Select data from Employee table \ulnone \
\ul employeelnfo = sqlContext.sql("select * from EmployeeTab") \
#lterate data and print \ulnone \
for row in employeelnfo.collect(): \
print(row) \

\b \ul Step 4 : Write data as a Text file 
\b0 \ulnone \
employeelnfo.toJSON().saveAsTextFile("employeeJson1") \
Step 5: Check whether data has been created or not \
hadoop fs -cat employeeJson1/part* \
----------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 89 
\b0 \ulnone : You have been given MySQ1DB with following details.\
user=retail_dba  password=cloudera  database=retail_db table=retail_db.orders table=retail_db.order_items jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Columns of order table : (order_id, order_date, order_customer_id, order_status)\
Columns of order_items table : (order_item_id, order_item_order_id, order_item_product_id, order_item_quantity,order_item_subtotal,order_item_product_price)\
Please accomplish following activities.\
1. Copy "retail_db.orders" and "retail_db.order_items" table to hdfs in respective directory p89_orders and p89_order_items.\
2. Join these data using order_id in spark and Python\
3. Now fetch selected columns from joined data OrderId, Order date and amount collected on this order.\
4. Calculate tota1order placed for each date, and produced the output sorted by date.\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Import Single table
\b0 \ulnone  . \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera -table=orders --target-dir=p89_orders --m 1 \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera -table=order_items --target-dir=p89_order_items --m 1 \
Sqoop uses the MapReduce framework to copy data from RDBMS to hdfs \

\b \ul Step 2 : Read the data from one of the partition, created using above command
\b0 \ulnone . \
hadoop fs -cat p89_orders/part-m-OOOOO \
hadoop fs -cat p89_order_items/part-m-OOOOO \

\b \ul Step 3 : Load these above two directory as RDD using Spark and Python (Open pyspark termina1and do following)
\b0 \ulnone . \
orders = sc.textFile("p89_orders") \
orderltems = sc.textFile("p89_order_items") \

\b \ul Step 4 : Convert RDD into key value as (order_id as a key and rest of the values as a value)
\b0 \ulnone  \
\ul #First value is order_id \ulnone \
ordersKeyValue = orders.map(lambda line:(int(line.spilt(",")[0),line)) \
\ul #Second value as an Order_id \ulnone \
orderltemsKeyValue = orderltems.map(lambda line: (int(line.split(\'93,\'94)[1]),line))\

\b \ul Step 5 : Join both the RDD using order_id 
\b0 \ulnone \
joinedData = orderltemsKeyValue.join(ordersKeyValue) \
\ul #print the joined data\ulnone  \
for line in joinedData.collect(): \
print(line) \
Format of joinedData as below. \
[Orderld, 'Al1columns from orderltemsKeyValue' , 'Al1columns from ordersKeyValue'] \

\b \ul Step 6 : Now fetch selected values Orderld, Order date and amount collected on this order
\b0 \ulnone . \
revenuePerOrderPerDay = joinedData.map(lambda row: (row[O],row[1][1].split(",")[1],float(row[1][0].split(",")[4])))\
\ul #print the result \ulnone \
for line in revenuePerOrderPerDay.collect(): \
print(line) \

\b \ul Step 7 : Select distinct order ids for each date. 
\b0 \ulnone \
\ul #distinct(date,order_id) \ulnone \
distinctOrdersDate = joinedData.map(lambda row:row[1][1].spilt(",")[1]+","+str(row[0])).distinct()\
for line in distinctOrdersDate.collect(): \
print(line) \

\b \ul Step 8 : Similar to word count , generate (date, 1) record for each row
\b0 \ulnone . \
newLineTuple = distinctOrdersDate.map(lambda line:(line.split(",")[0], 1)) \

\b \ul Step 9 : Do the count for each key(date), to get tota1order per date
\b0 \ulnone . \
totalOrdersPerDate = newLineTuple.reduceByKey(lambda a, b: a + b) \
\ul #print results \ulnone \
for line in totalOrdersPerDate.collect(): \
print(line) \

\b \ul Step 10 : Sort the results by date 
\b0 \ulnone \
\ul sortedData=totalOrdersPerDate.sortByKey().collect() \
#print results\ulnone \
for line in sortedData:\
print(line)\
\
-----------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 90 :
\b0 \ulnone  You have been given MySQ1DB with following details. (You need to know al1these functions and its behaviour for rea1Thoughtworks)\
user=retail_dba password=cloudera database=retail_db table=retail_db.orders table=retail_db.order_items jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Please accomplish following activities.\
1. Copy "retail_db.order_items" table to hdfs in respective directory p90_order_items.\
2. Do the summation of entire revenue in this table using pyspark.\
3. Find the maximum and minimum revenue as well.\
4. Calculate average revenue\
Columns of order_items table : (order_item_id, order_item_order_id, order_item_product_id, order_item_quantity, order_item_subtotal, order_item_product_price)\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Import Single table 
\b0 \ulnone . \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba--password=cloudera \'97table=order_items --target-dir=p90_order_items --m 1 \
Sqoop uses the MapReduce framework to copy data from RDBMS to hdfs \

\b \ul Step 2 : Read the data from one of the partition, created using above command
\b0 \ulnone . \
hadoop fs -cat p90_order_items/part-m-OOOOO \

\b \ul Step 3 : In pyspark, get the tota1revenue across al1days and orders.
\b0 \ulnone  \
entireTableRDD = sc.textFile("p90_order_items") \

\b \ul #Cast string to float
\b0 \ulnone  \
extractedRevenueColumn = entireTableRDD.map(lambda line:float(line.split(",")[4]))\

\b \ul Step 4 : Verity extracted data 
\b0 \ulnone \
for revenue in extractedRevenueColumn.collect(): \
print revenue \
\ul #use reduce function to sum a single column vale\ulnone  \
totalRevenue = extractedRevenueColumn.reduce(lambda a, b: a + b) \

\b \ul Step 5 : Calculate the maximum revenue 
\b0 \ulnone \
maximumRevenu\'e9=extractedRevenueColumn.reduce(lambda a, b: (a if a>=b else b) ) \

\b \ul Step 6 : Calculate the minimum revenue 
\b0 \ulnone \
minimumRevenue = extractedRevenueColumn.reduce(lambda a, b: (a if a<=b else b) ) \

\b \ul Step 7 : Caclculate average revenue
\b0 \ulnone \
count=extratedRevenueColumn.count()\
averageRev=totalRevenue/count \
-------------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 91 
\b0 \ulnone : You have been given MySQ1DB with following details.\
user=retail_dba password=cloudera  database=retail_db table=retail_db.orders table=retail_db.order_items jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Columns of order table : (order_id, order_date, order_customer_id, order_status)\
\
Please accomplish following activities.\
1. Copy "retail_db.orders" table to hdfs in a directory p91_orders\
2. Once data is copied to hdfs, using pyspark calculate the number of order for each status.\
3. Use al1the following methods to calculate the number of order for each status\
-countByKey()\
-groupByKey()\
-reduceByKey()\
-aggregateBykey()\
-combinBykey()\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Import Single table
\b0 \ulnone  \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username-retail_dba --password=cloudera \'97table=orders --target-dir=p91_orders --m1\
Sqoop uses the MapReduce framework to copy data trom RDBMS to hdfs \

\b \ul Step 2 : Read the data from one of the partition, created using above command
\b0 \ulnone . \
hadoop fs -cat p91_orders/part-m-OOOOO \

\b \ul Step 3 : countByKey 
\b0 \ulnone \
#Number of orders by status \
allOrders = sc.textFile("p91_orders") \
#\ul Generate key and value pairs (key is order status and vale as an empty string\ulnone  \
keyValue = allOrders.map(lambda line:(line.split(",")[3], "")) \
#\ul Using countByKey, aggregate data based on status as a key\ulnone  \
output=keyValue.countByKey().items() \
for line in output : print(line) \

\b \ul Step 4 : groupByKey 
\b0 \ulnone \
#\ul Generate key and value pairs (key is order status and vale as an one\ulnone  \
keyValue = allOrders.map(lambda line: (line.split(",")[3],1)) \
#\ul Using countByKey, aggregate data based on status as a key \
\ulnone output= keyValue.groupByKey().map(lambda kv: (kv[O], sum(kv[l]))) \
for line in output.collect() : print(line) \

\b \ul  Step 5 : reduceByKey 
\b0 \ulnone \
#\ul Generate key and value pairs (key is order status and vale as an one\ulnone  \
keyValue = allOrders.map(lambda line: (line.split(",")[3],1)) \
#\ul Using countByKey, aggregate data based on status as a key\ulnone  \
output= keyValue.reduceByKey(lambda a, b: a + b)\
for line in output.collect() : print(line) \

\b \ul Step 6 : aggregateByKey 
\b0 \ulnone \
#\ul Generate key and value pairs (key is order status and vale as an one\ulnone  \
keyValue = allOrders.map(lambda line:(line.split(",")[3],line)) \
output=keyValue.aggregateByKey(0, lambda a, b: a+s, lambda a, b: a+b) \
for line in output.collect() : print(line) \

\b \ul Step 7 : combineByKey 
\b0 \ulnone \
#\ul Generate key and value pairs (key is order status and vale as an one\ulnone  \
keyValue = allOrders.map(lambda line:(line.split(",")[3], line)) \
output=keyValue.combineByKey(lambda value: 1, lambda acc, value: acc+1 lambda acc, value: acc+value) \
for line in output.collect() : print(line) \
\
(These are very important functions for the exam - Thought works) \
---------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 92
\b0 \ulnone  : You have been given MySQ1DB with following details.\
user=retail_dba password=cloudera database=retail_db table=retail_db.orders table=retail_db.order_items jdbcURL= jdbc:mysql://quickstart:3306/retail_db\
Columns of order table : (order_id, order_date, order_customer_id, order_status)\
Columns of order_items table : (order_item_id, order_item_order_id, order_item_product_id, order_item_quantity,order_item_subtotal,order_item_product_price)\
Please accomplish following activities.\
\
1. Copy "retail_db.orders" and "retail_db.order_items" table to hdfs in respective directory p92_orders and p92_order_items.\
2. Join these data using order_id in Spark and Python\
3. Calculate total revenue per day and per order\
4. Calculate total and average revenue for each date.\
-combineByKey\
-aggregateByKey\

\b \ul \
Solution:
\b0 \ulnone \
\

\b \ul Step 1 : Import Single table . 
\b0 \ulnone \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera \'97table=orders --target-dir=p92_orders --m 1 \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera \'97table=order_items --target-dir=p92_order_items --m 1 \
Note : Please check you dont have space between before or after'='sign. \
Sqoop uses the MapReduce framework to copy data from RDBMS to hdfs \

\b Step 2 : Read the data from one of the partition, created using above command. 
\b0 \
hadoop fs -cat p92_orders/part-m-OOOOO \
hadoop fs -cat p92_order_items/part-m-OOOOO \

\b \ul Step 3 : Load these above two directory as RDD using Spark and Python (Open pyspark termina1and do following).
\b0 \ulnone  \
orders = sc.textFile("p92_orders") \
orderltems = sc.textFile("p92_order_items") \

\b \ul Step 4 : Convert RDD into key value as (order_id as a key and rest of the values as a value)\ulnone  \

\b0 #First value is order_id \
ordersKeyValue = orders.map(lambda line:(int(line.split(",")[0]), line)) \
#Second value as an Order_id\
orderltemsKeyValue = orderltems.map(lambda line: (int(line.split(",")[1]),line)) \

\b \ul Step 5 : Join both the RDD using order_id
\b0 \ulnone  \
joinedData = orderltemsKeyValue.join(ordersKeyValue) \
#print the joined data \
for line in joinedData.collect(): \
print(line) \
Format of joinedData as below. \
[
\b \ul Orderld, 'Al1columns from orderltemsKeyValue' , 'Al1columns from ordersKeyValue'
\b0 \ulnone ] \

\b \ul Step 6 : Now fetch selected values Orderld, Order date and amount collected on this order
\b0 \ulnone . \
//Returned row wil1contain ((order_date,order_id),amount_collected) \
revenuePerDayPerOrder = joinedData.map(lambda row:((row[1][1].split(",")[1],row[0]),float(row[1][0].split(",")[4]))) \
#print the result \
for line in revenuePerDayPerOrder.collect(): \
print(line) \

\b \ul Step 7 : Now calculate tota1revenue per day and per order 
\b0 \ulnone \
A. using reduceByKey \
totalRevenuePerDayPerOrder = revenuePerDayPerOrder.reduceByKey(lambda runningSum, value: runningSum + value) \
for line in totalRevenuePerDayPerOrder.sortByKey().collect(): \
print(line) \
#\ul Generate data as (date, amount_collected) (Ignore order_id)\ulnone  \
dateAndRevenueTuple = totalRevenuePerDayPerOrder.map(lambda line:(line[0][0],line[1])) \
for line in dateAndRevenueTuple.sortByKey().collect(): \
print(line) \

\b \ul Step 8 : Calculate tota1amount collected for each day. And also calculate number of days. 
\b0 \ulnone \
#Generate output as (Date, Tota1Revenue for date, total_number_of_dates) \
#Line 1:it wil1generate tuple(revenue,1)\
#Line 2:Here,we wil1do summation for al1revenues at the same time another counter to maintain number of records.\
#Line 3:Fina1function to merge al1the combiner\
totalRevenueAndTotalCount=dateAndRevenueTuple.combineByKey(lambda revenue:(revenue,1),lambda revenueSumTuple.amount:(revenueSumTuple[0]+amount,revenueSumTuple[1]+1),lambda tuple1,tuple2:(round(tuple1[0]+tuple2[0],2),tuple1[1]+tuple2[1]))\
for line in totalRevenueAndTotalCount.collect();\
print(line)\

\b \ul Step 9:Now calculate average for each date
\b0 \ulnone \
averageRevenuePerDate=totalRevenueAndTotalCount.map(lambda threeElements:(threeElements[0],threeElements[1][0]/threeElements[1][1]))\
for line in averageRevenuePerDate.collect():\
print(line)\

\b \ul Step 10:Using aggregateByKey
\b0 \ulnone \
#line 1:(initialize both the value,revenue and court)\
#line 2:runningRevenueSumTuple(Its a tuple for tota1revenue and record count for each date)\
#line 3:Summing al1partitions revenuue and count\
totalRevenueAndTotalCount=dateAndRevenueTuple.aggregateByKey((0,0),lambda runningRevenueSumTuple,revenue:(runningRevenueSumTuple[0]+revenue,runningRevenueSumTuple[1]+1),lambda tupleOneRevenueAndCount,tupleTwoRevenueAndCount:(tupleOneRevenueAndCount[0]+tupleTwoRevenueAndCount[0],tupleOneRevenueAndCount[1]+tupleTwoRevenueAndCount[1]))\
for line in totalRevenueAndTotalCount.collect():\
print(line)\

\b \ul Step 11 : Calculate the average revenue per date 
\b0 \ulnone \
averageRevenuePerDate = totalRevenueAndTotalCount.map(lambda threeElements: (threeElements[O],threeElements[1][0]/threeElements[1][1]))\
for line in averageRevenuePerDate.collect(): \
print(line) \
------------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 93
\b0 \ulnone  : You have been given MySQ1DB with following details.\
user=retail_dba password=cloudera database=retail_db table=retail_db.orders table=retail_db.order_item jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Columns of order table : (order_id, order_data, order_customer_id, order_status)\
Columns of order_items table : (order_item_id, order_item_order_id, order_item_product_id, order_item_quantity,order_item_subtotal,order_item_product_price)\
Please accomplish following activities.\
1. Copy "retail_db.orders" and "retail_db.order_items" table to hdfs in respective directory p92_orders and p92_order_items.\
2. Join these data using order_id in Spark and Python\
3. Calculate tota1revenue per day and per order\
4. Calculate maximum revenue customer\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Import Single table .
\b0 \ulnone  \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera  \'97table=orders --target-dir=p92_orders --m 1 \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera   \'97table=order_items --target-dir=p92 _order_items --m 1\
Note : Please check you dont have space between before or after'=' sign. \
Sqoop uses the MapReduce framework to copy data from RDBMS to hdfs \

\b \ul Step 2 : Read the data from one of the partition, created using above command. 
\b0 \ulnone \
hadoop fs -cat p92_orders/part-m-OOOOO \
hadoop fs -cat p92_order_items/part-m-OOOOO \

\b \ul Step 3 : Load these above two directory as RDD using Spark and Python (Open pyspark termina1and do following). 
\b0 \ulnone \
orders = sc.textFile("p92_orders") \
orderltems = sc.textFile("p92_order_items") \

\b \ul Step 4 : Convert RDD into key value as (order_id as a key and rest of the values as a value) 
\b0 \ulnone \
\ul #First value is order_id \ulnone \
ordersKeyValue = orders.map(lambda line:(int(line.split(\'93,\'94)[0]), line)) \
\ul #Second value as an Order_id \ulnone \
orderltemsKeyValue = orderltems.map(lambda line: (int(line.split(",")[1]),line)) \

\b \ul Step 5 : Join both the RDD using order_id 
\b0 \ulnone \
joinedData = orderltemsKeyValue.join(ordersKeyValue) \
\ul #print the joined data \ulnone \
for line in joinedData.collect(): \
print(line) \
\ul #Format of joinedData as below\ulnone . \
\ul #[OrderId,'Al1columns from orderltemsKeyValue' , 'Al1columns from ordersKeyValue']\ulnone  \
ordersPerDatePerCustomer = joinedData.map(lambda line:((line[1][1].split(",")[1],line[1][1].split(",")[2]),float(line[1][0].split(",")[4])))\
amountCollectedPerDayPerCustomer = ordersPerDatePerCustomer.reduceByKey(lambda runningSum, amount: runningSum + amount) \
\ul #(Out record format wil1be ((date,customer_id), totalAmount)\ulnone  \
for line in amountCollectedPerDayPerCustomer.collect(): \
print(line) \
\ul #now change the format of record as (date,(customer_id,total_amount))\ulnone  \
revenuePerDatePerCustomerRDD = amountCollectedPerDayPerCustomer.map(lambda threeElementTuple: (threeElementTuple[0][0],(threeElementTuple[0][1],threeElementTuple[1])))\
for line in revenuePerDatePerCustomerRDD.collect(): \
print(line) \
\ul #Calculate maximum amount collected by a customer for each day \ulnone \
perDateMaxAmountCollectedByCustomer = revenuePerDatePerCustomerRDD.reduceByKey(lambda runningAmountTuple, newAmountTuple:(runningAmountTuple if runningAmountTuple[1]>=newAmountTuple[1]else newAmountTuple))\
 for line in perDateMaxAmountCollectedByCustomer.sortByKey().collect(): \
print(line) \
-------------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 94
\b0 \ulnone  : You have been given MySQ1DB with following details.\
user=retail_dba password=cloudera  database=retail_db 
\b \ul table=retail_db.orders table=retail_db.order_items 
\b0 \ulnone jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Columns of products table : (product_id | product_category_id | product_name | product_desctiption | product_price | product_image)\
Please accomplish following activities.\
\
1. Copy "retail_db.products" table to hdfs in a directory p93_products\
2. Filter out al1the empty prices\
3. Sort al1the products based on price in both ascending as wel1as descending order.\
4. Sort al1the products based on price as wel1as product_id in descending order\
5. Use the below functions to do data ordering or ranking and fetch top 10 elements\
top()\
takeOrdered()\
sortByKey()\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Import Single table . 
\b0 \ulnone \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera \'97table=products --target-dir=p93_products --m 1\
 Note : Please check you don\'92t have space between before or after'='sign. \
Sqoop uses the MapReduce framework to copy data from RDBMS to hdfs \

\b \ul Step 2 : Read the data from one of the partition, created using above command. 
\b0 \ulnone \
hadoop fs -cat p93_products/part-m-OOOOO \

\b \ul Step 3 : Load this directory as RDD using Spark and Python (Open pyspark termina1and do following). 
\b0 \ulnone \
productsRDD = sc.textFile("p93_products") \

\b \ul Step 4 : Filter empty prices, it exists 
\b0 \ulnone \
#filter out empty prices lines \
nonempty_lines = productsRDD.filter(lambda x:len(x.split(",")[4]) > O) \

\b \ul Step 5 : Now sort data based on product_price in order.
\b0 \ulnone  \
sortedPriceProducts=nonempty_lines.map(lambda line :(float(line.split(",")[4]),line.split(",")[2])).sortByKey() \
for line in sortedPriceProducts.collect(): \
print(line) \

\b \ul Step 6 : Now sort data based on product_price in descending order. 
\b0 \ulnone \
sortedPriceProducts=nonempty_lines.map(lambda line :(float(line.split(",")[4]),line.split(",")[2])).sortByKey(False) \
for line in sortedPriceProducts.collect(): \
print(line) \

\b \ul Step 7 : Get highest price products name. 
\b0 \ulnone \
sortedPriceProducts=nonempty_lines.map(lambda line :(float(line.split(",")[4]),line.split(",")[2])).sortByKey(False).take(1)  \
print(sortedPriceProducts) \

\b \ul Step 8 : Now sort data based on product_price as wel1as product_id in descending order.
\b0 \ulnone  \
#Don\'92t forget to cast string \
#Tuple as key ((price,id),name) \
sortedPriceProducts=nonempty_lines.map(lambda line :((float(line.split(",")[4]),int(line.split(",")[0])),line.split(",")[2])).sortByKey(False).take(10)\
print(sortedPriceProducts) \

\b \ul Step 9 : Now sort data based on product_price as wel1as product_id in descending order, using top() function.
\b0 \ulnone  \
#Dont torget to cast string \
#Tuple as key ((price,id),name) \
sortedPriceProducts=nonempty_lines.map(lambda line :((float(line.split(",")[4]),int(line.split(",")[0])),line.split(",")[2])).top(10) \
print(sortedPriceProducts) \

\b \ul Step 10 : Now sort data based on product_price as ascending and product_id in ascending order, using takeordered() function. 
\b0 \ulnone \
#Dont forget to cast string \
#Tuple as key ((price,id),name)\
sortedPriceProducts=nonempty_lines.map(lambda line :((float(line.split(\'93,\'94)[4]),int(line.split(",")[0])),line.split(",")[2])).takeOrdered(10,lambda tuple:(tuple[0][0],tuple[0][1]))\

\b \ul  Step 11 : Now sort data based on product_price as descending and product_id in ascending order, using takeordered() function. 
\b0 \ulnone \
#Don\'92t forget to cast string \
#Tuple as key ((price,id),name) \
#Using minus(-) parameter can help you to make descending ordering , only tor numeric value. \
sortedPriceProducts=nonempty_lines.map(lambda line :((float(line.split(\'93,\'94)[4]),int(line.split(",")[0])),line.split(",")[2])).takeOrdered(10,lambda tuple:(-tuple[0][0],tuple[0][1])) \
------------------------------------------------------------------------------------------------------------------------------------\

\b \ul Problem Scenario 95 : 
\b0 \ulnone You have been given MySQ1DB with following details.\
user=retail_dba password=cloudera database=retail_db table=retail_db.orders table=retail_db.order_items jdbcUR1= jdbc:mysql://quickstart:3306/retail_db\
Columns of products table : (product_id | product_category_id | product_name | product_desctiption | product_price | product_image)\
Please accomplish following activities.\
\
1. Copy "retail_db.products" table to hdfs in a directory p93_products\
2. Now sort the products data sorted by product price per category, use product_category_id column to group by category\
\

\b \ul Solution:
\b0 \ulnone \

\b \ul Step 1 : Import Single table . 
\b0 \ulnone \
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera \'97table=products --target-dir=p93_products --m 1\
Note : Please check you don\'92t have space between before or after'=' sign. \
Sqoop uses the MapReduce framework to copy data from RDBMS to hdfs \

\b \ul Step 2 :Read the data from one of the partition, created using above command
\b0 \ulnone . \
hadoop fs -cat p93_products/part-m-OOOOO \

\b \ul Step 3 : Load this directory as RDD using Spark and Python (Open pyspark termina1and do following).
\b0 \ulnone  \
productsRDD = sc.textFile("p93_products") \

\b \ul Step 4 : Filter empty prices, it exists 
\b0 \ulnone \
#filter out empty prices lines \
nonempty_lines = productsRDD.filter(lambda x: len(x.split(",")[4])>0) \

\b \ul Step 5 : Create data set like (categoryld, (id,name,price
\b0 \ulnone ) \
mappedRDD = nonempty_lines.map(lambda line: (line.split(",")[1],(line.split(",")[0],line.split(",")[2],float(line.split(",")[4]))) \
for line in mappedRDD.collect(): print(line)  \

\b \ul Step 6 : Now groupBy the al1records based on categoryld, which a key on mappedRDD it wil1produce output like (categoryld, iterable of al1lines for a key/categoryId)
\b0 \ulnone \
groupByCategroyld = mappedRDD.groupByKey() \
for line in groupByCategroyld.collect(): print(line) \

\b \ul Step 7 : Now sort the data in each category based on price in ascending order. 
\b0 \ulnone \
# sorted is a function to sort an Iterable, we can also specify, what would be the key on which we want to sort in this case we have price on which it needs to be sorted \
groupByCategroyld.map(lambda tuple: sorted(tuple[1], key=lambda tupleValue: tupleValue[2])).take(5) \

\b \ul Step 8 : Now sort the data in each category based on price in descending order
\b0 \ulnone . \
# sorted is a function to sort an Iterable, we can also specify, what would be the key on which we want to sort in this case we have price on which it needs to be sorted \
groupByCategroyld.map(lambda tuple: sorted(tuple[l1, key=lambda tupleValue: tupleValue[2] , reverse=True)).take(5) \
\
}